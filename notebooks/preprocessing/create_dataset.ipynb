{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pytz\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def readData(news_dir):\n",
    "    data_text = []\n",
    "    data_published = []\n",
    "    date = []\n",
    "    time = []\n",
    "    site = []\n",
    "    for folder in os.scandir(news_dir):\n",
    "        print(os.scandir(news_dir),type(os.scandir(news_dir)))\n",
    "        if folder.name!=\".DS_Store\":\n",
    "            for entry in os.scandir(news_dir+folder.name):\n",
    "                data = json.load(open(news_dir+folder.name+\"/\"+entry.name, encoding='utf-8'))\n",
    "                if data['published']!=\"\":\n",
    "                    data_published.append(data['published'])\n",
    "                    date_time = datetime.datetime.strptime(data['published'], \"%Y-%m-%dT%H:%M:%S.%f%z\").astimezone(pytz.utc)\n",
    "                    date.append(str(date_time.date()))\n",
    "                    time.append(str(date_time.time()))\n",
    "                else:\n",
    "                    data_published.append(\"No Value\")\n",
    "                    date.append(\"No Value\")\n",
    "                    time.append(\"No Value\")\n",
    "                if data['text']!=\"\":\n",
    "                    data_text.append(data['text'].lower())\n",
    "                else:\n",
    "                    data_text.append(\"no value\")\n",
    "                    \n",
    "                if data['thread']['site_full'] != '':\n",
    "                    site.append(data['thread']['site_full'])\n",
    "                else:\n",
    "                    site.append(\"unknown\")\n",
    "    news_df=pd.DataFrame({'date_time':data_published, 'text':data_text, 'day':date, 'time':time, 'site':site})\n",
    "    return news_df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "news_directory = \"../../Data/News/\"\n",
    "\n",
    "news_df = readData(news_directory)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "news_df.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "news_df.sort_values([\"day\", \"time\"],axis = 0, ascending=[True, True], inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "news_df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sites = news_df.site.unique()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sorted(sites)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def get_sentences(paragraph):\n",
    "    sentences = []\n",
    "    \n",
    "    first_split = sent_tokenize(paragraph)\n",
    "    \n",
    "    for maybe_sentences in first_split:\n",
    "        our_sentences = maybe_sentences.split(\"\\n\")\n",
    "        sentences.extend(our_sentences)\n",
    "        \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "news_df['sentences'] = news_df.text.apply(get_sentences)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "news_df['n_sentences'] = news_df.sentences.apply(lambda s_list: len(s_list))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "news_df.n_sentences.describe()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sentences_df = news_df.drop(columns=[\"n_sentences\",\"text\"]).explode('sentences').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sentences_df = sentences_df.dropna(subset=['sentences'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "amazon_words = ['amazon', 'amzn']\n",
    "amazon_news_df = sentences_df[sentences_df.sentences.str.contains(\"|\".join(amazon_words))]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "apple_words = ['apple', 'aapl']\n",
    "apple_news_df = sentences_df[sentences_df.sentences.str.contains(\"|\".join(apple_words))]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import unicodedata\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "from nltk.stem import LancasterStemmer,PorterStemmer\n",
    "\n",
    "\n",
    "def nonAsciiChar(words):\n",
    "    words_list=[]\n",
    "    for w in words:\n",
    "        w=re.sub('[^a-zA-Z]+','',re.sub(r'[\\W\\d]','',w.lower()))\n",
    "        format_words=unicodedata.normalize('NFKD', w).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        words_list.append(format_words)\n",
    "        \n",
    "    return words_list\n",
    "def stemWordsRemoval(words):\n",
    "    stemmer = PorterStemmer()\n",
    "    words_list=[]\n",
    "    for word in words:\n",
    "        word=stemmer.stem(word)\n",
    "        if word not in words_list:\n",
    "            words_list.append(word)\n",
    "    return words_list\n",
    "\n",
    "def stopWordsRemoval(words):\n",
    "    words_list=[]\n",
    "    for w in words:\n",
    "        if w not in stopwords:\n",
    "            words_list.append(w)\n",
    "    return words_list\n",
    "\n",
    "def removeLinks(words):\n",
    "    words_list=[]\n",
    "    for w in words:\n",
    "        if not re.match('[www]',w):\n",
    "            words_list.append(w)\n",
    "    return words_list\n",
    "\n",
    "def spaceRemoval(words):\n",
    "    words_list=[]\n",
    "    for w in words:\n",
    "        if w!='':\n",
    "            words_list.append(w)\n",
    "    return words_list\n",
    "\n",
    "\n",
    "def dataExtraction(words):\n",
    "    words=nonAsciiChar(words)\n",
    "    words=spaceRemoval(words)\n",
    "    words=stopWordsRemoval(words)\n",
    "    words=stemWordsRemoval(words)\n",
    "    words=removeLinks(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def process_sentences(sentences_df):\n",
    "    sentences_df=sentences_df[sentences_df['sentences'].str.match('^[A-Z a-z 0-9]+')]\n",
    "    sentences_df=sentences_df.drop_duplicates(keep=False).reset_index(drop=True)\n",
    "    sentences_df['words'] = sentences_df.sentences.apply(word_tokenize)\n",
    "    sentences_df['words'] = sentences_df.words.apply(dataExtraction)\n",
    "    sentences_df['text'] = sentences_df.words.apply(lambda words: \" \".join(words))\n",
    "    sentences_df = sentences_df.drop(columns=[\"words\",\"sentences\"])\n",
    "    return sentences_df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "amazon_news_df = process_sentences(amazon_news_df)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "apple_news_df = process_sentences(apple_news_df)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def load_stock_price_dataset(path):\n",
    "    stock_df=pd.read_csv(path,names=['day','time','x1','price','x2','x3','x4'])[['day','time','price']]\n",
    "    stock_df.day = stock_df.day.apply(lambda s:s.replace('.','-'))\n",
    "    return stock_df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "amazon_stock_price_60 = load_stock_price_dataset('../../Data/CHARTS/AMAZON60.csv')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "apple_stock_price_60 = load_stock_price_dataset('../../Data/CHARTS/APPLE60.csv')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def assign_labels(news_df, stocks_df):\n",
    "    labels_df = pd.DataFrame(columns=[\"text\", \"label\", \"day\", \"time\", \"site\"])\n",
    "    for row_index in tqdm(range(len(news_df))):\n",
    "        row = news_df.iloc[row_index]\n",
    "        \n",
    "        day = row.day\n",
    "        time = row.time\n",
    "        text = row.text\n",
    "        \n",
    "#         print(day, time)\n",
    "        next_time_step_indexes = stocks_df[(stocks_df.day == day) & (stocks_df.time > time)].index\n",
    "        if len(next_time_step_indexes) != 0:\n",
    "            next_time_step_index = next_time_step_indexes[0]\n",
    "#             print(\"pass\")\n",
    "        else:\n",
    "            next_time_step_indexes = stocks_df[(stocks_df.day > day)].index\n",
    "            if len(next_time_step_indexes) != 0:\n",
    "                next_time_step_index = next_time_step_indexes[0]\n",
    "#                 print(\"pass\")\n",
    "            else:\n",
    "#                 print(\"fail\")    \n",
    "                continue\n",
    "        prev_time_step_index = next_time_step_index - 1 if next_time_step_index>0 else 0\n",
    "        \n",
    "        label = 1 if stocks_df.iloc[next_time_step_index].price >= stocks_df.iloc[prev_time_step_index].price else 0\n",
    "        labels_df.loc[len(labels_df)] = [text, label, day, time, row.site]\n",
    "        \n",
    "        \n",
    "    return labels_df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "amazon_labels = assign_labels(amazon_news_df, amazon_stock_price_60)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "apple_labels = assign_labels(apple_news_df, apple_stock_price_60)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "amazon_news_df.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "apple_news_df.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "amazon_labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fresh start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\arpit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\arpit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pytz\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "import unicodedata\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "from nltk.stem import LancasterStemmer,PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(news_dir):\n",
    "    data_text = []\n",
    "    data_published = []\n",
    "    date = []\n",
    "    time = []\n",
    "    site = []\n",
    "    for folder in os.scandir(news_dir):\n",
    "        print(os.scandir(news_dir),type(os.scandir(news_dir)))\n",
    "        if folder.name!=\".DS_Store\":\n",
    "            for entry in os.scandir(news_dir+folder.name):\n",
    "                data = json.load(open(news_dir+folder.name+\"/\"+entry.name, encoding='utf-8'))\n",
    "                if data['published']!=\"\":\n",
    "                    data_published.append(data['published'])\n",
    "                    date_time = datetime.datetime.strptime(data['published'], \"%Y-%m-%dT%H:%M:%S.%f%z\").astimezone(pytz.utc)\n",
    "                    date.append(str(date_time.date()))\n",
    "                    time.append(str(date_time.time()))\n",
    "                else:\n",
    "                    data_published.append(\"No Value\")\n",
    "                    date.append(\"No Value\")\n",
    "                    time.append(\"No Value\")\n",
    "                if data['text']!=\"\":\n",
    "                    data_text.append(data['text'].lower())\n",
    "                else:\n",
    "                    data_text.append(\"no value\")\n",
    "                    \n",
    "                if data['thread']['site_full'] != '':\n",
    "                    site.append(data['thread']['site_full'])\n",
    "                else:\n",
    "                    site.append(\"unknown\")\n",
    "    news_df=pd.DataFrame({'date_time':data_published, 'text':data_text, 'day':date, 'time':time, 'site':site})\n",
    "    return news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<nt.ScandirIterator object at 0x000001B2B6E6D560> <class 'nt.ScandirIterator'>\n",
      "<nt.ScandirIterator object at 0x000001B2B6E71CE0> <class 'nt.ScandirIterator'>\n",
      "<nt.ScandirIterator object at 0x000001B2B6E73860> <class 'nt.ScandirIterator'>\n",
      "<nt.ScandirIterator object at 0x000001B2B6E727E0> <class 'nt.ScandirIterator'>\n",
      "<nt.ScandirIterator object at 0x000001B2B6E727E0> <class 'nt.ScandirIterator'>\n",
      "<nt.ScandirIterator object at 0x000001B2B6E727E0> <class 'nt.ScandirIterator'>\n",
      "<nt.ScandirIterator object at 0x000001B2B6E727E0> <class 'nt.ScandirIterator'>\n",
      "<nt.ScandirIterator object at 0x000001B2B6E727E0> <class 'nt.ScandirIterator'>\n",
      "<nt.ScandirIterator object at 0x000001B2B6E727E0> <class 'nt.ScandirIterator'>\n",
      "<nt.ScandirIterator object at 0x000001B2B6E727E0> <class 'nt.ScandirIterator'>\n",
      "<nt.ScandirIterator object at 0x000001B2B6E727E0> <class 'nt.ScandirIterator'>\n",
      "<nt.ScandirIterator object at 0x000001B2B6E727E0> <class 'nt.ScandirIterator'>\n",
      "<nt.ScandirIterator object at 0x000001B2B6E727E0> <class 'nt.ScandirIterator'>\n",
      "<nt.ScandirIterator object at 0x000001B2B6E727E0> <class 'nt.ScandirIterator'>\n"
     ]
    }
   ],
   "source": [
    "news_directory = \"../../Data/News/\"\n",
    "\n",
    "news_df = readData(news_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.sort_values([\"day\", \"time\"],axis = 0, ascending=[True, True], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stock_price_dataset(path):\n",
    "    stock_df=pd.read_csv(path,names=['day','time','x1','price','x2','x3','x4'])[['day','time','price']]\n",
    "    stock_df.day = stock_df.day.apply(lambda s:s.replace('.','-'))\n",
    "    return stock_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_stock_price_60 = load_stock_price_dataset('../../Data/CHARTS/AMAZON60.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_stock_price_60 = load_stock_price_dataset('../../Data/CHARTS/APPLE60.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(stocks_df, day, time):\n",
    "    next_time_step_indexes = stocks_df[(stocks_df.day == day) & (stocks_df.time > time)].index\n",
    "    if len(next_time_step_indexes) != 0:\n",
    "        next_time_step_index = next_time_step_indexes[0]\n",
    "    #    print(\"pass\")\n",
    "    else:\n",
    "        return np.nan\n",
    "        next_time_step_indexes = stocks_df[(stocks_df.day > day)].index\n",
    "        if len(next_time_step_indexes) != 0:\n",
    "            next_time_step_index = next_time_step_indexes[0]\n",
    "    #       print(\"pass\")\n",
    "        else:\n",
    "    #       print(\"fail\")    \n",
    "            return np.nan\n",
    "    prev_time_step_index = next_time_step_index - 1 if next_time_step_index>0 else 0\n",
    "    \n",
    "    label = 1 if stocks_df.iloc[next_time_step_index].price >= stocks_df.iloc[prev_time_step_index].price else 0\n",
    "    return label\n",
    "\n",
    "\n",
    "def assign_labels(news_df, stocks_df_1, stocks_df_2):\n",
    "    labels_df = pd.DataFrame(columns=[\"text\", \"label_1\", \"label_2\", \"day\", \"time\", \"site\"])\n",
    "    for row_index in tqdm(range(len(news_df))):\n",
    "        row = news_df.iloc[row_index]\n",
    "        \n",
    "        day = row.day\n",
    "        time = row.time\n",
    "        text = row.text\n",
    "        \n",
    "        label_1 = get_label(stocks_df_1, day, time)\n",
    "        label_2 = get_label(stocks_df_2, day, time)\n",
    "        \n",
    "        labels_df.loc[len(labels_df)] = [text, label_1, label_2, day, time, row.site]\n",
    "        \n",
    "        \n",
    "    return labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 78055/78055 [29:44<00:00, 43.74it/s]\n"
     ]
    }
   ],
   "source": [
    "labels_df = assign_labels(news_df, amazon_stock_price_60, apple_stock_price_60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(paragraph):\n",
    "    sentences = []\n",
    "    \n",
    "    first_split = sent_tokenize(paragraph)\n",
    "    \n",
    "    for maybe_sentences in first_split:\n",
    "        our_sentences = maybe_sentences.split(\"\\n\")\n",
    "        sentences.extend(our_sentences)\n",
    "        \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df['sentences'] = labels_df.text.apply(get_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df['n_sentences'] = labels_df.sentences.apply(lambda s_list: len(s_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    78055.000000\n",
       "mean        41.844533\n",
       "std         43.179156\n",
       "min          0.000000\n",
       "25%         19.000000\n",
       "50%         35.000000\n",
       "75%         54.000000\n",
       "max       1964.000000\n",
       "Name: n_sentences, dtype: float64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_df.n_sentences.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_df = labels_df.drop(columns=[\"n_sentences\",\"text\"]).explode('sentences').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3266207, 6)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_df = sentences_df.dropna(subset=['sentences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "      <th>day</th>\n",
       "      <th>time</th>\n",
       "      <th>site</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>20:33:00</td>\n",
       "      <td>www.nephrologynews.com</td>\n",
       "      <td>annual rpa meeting emphasizes physician leader...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>20:33:00</td>\n",
       "      <td>www.nephrologynews.com</td>\n",
       "      <td>the renal physicians association has long held...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>20:33:00</td>\n",
       "      <td>www.nephrologynews.com</td>\n",
       "      <td>however, many practitioners do not possess ess...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>20:33:00</td>\n",
       "      <td>www.nephrologynews.com</td>\n",
       "      <td>to help remedy this situation, the rpa has pla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>20:33:00</td>\n",
       "      <td>www.nephrologynews.com</td>\n",
       "      <td>this workshop will be co-facilitated by rpa pr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label_1 label_2         day      time                    site  \\\n",
       "0       0       0  2018-01-03  20:33:00  www.nephrologynews.com   \n",
       "1       0       0  2018-01-03  20:33:00  www.nephrologynews.com   \n",
       "2       0       0  2018-01-03  20:33:00  www.nephrologynews.com   \n",
       "3       0       0  2018-01-03  20:33:00  www.nephrologynews.com   \n",
       "4       0       0  2018-01-03  20:33:00  www.nephrologynews.com   \n",
       "\n",
       "                                           sentences  \n",
       "0  annual rpa meeting emphasizes physician leader...  \n",
       "1  the renal physicians association has long held...  \n",
       "2  however, many practitioners do not possess ess...  \n",
       "3  to help remedy this situation, the rpa has pla...  \n",
       "4  this workshop will be co-facilitated by rpa pr...  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonAsciiChar(words):\n",
    "    words_list=[]\n",
    "    for w in words:\n",
    "        w=re.sub('[^a-zA-Z]+','',re.sub(r'[\\W\\d]','',w.lower()))\n",
    "        format_words=unicodedata.normalize('NFKD', w).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        words_list.append(format_words)\n",
    "        \n",
    "    return words_list\n",
    "def stemWordsRemoval(words):\n",
    "    stemmer = PorterStemmer()\n",
    "    words_list=[]\n",
    "    for word in words:\n",
    "        word=stemmer.stem(word)\n",
    "        if word not in words_list:\n",
    "            words_list.append(word)\n",
    "    return words_list\n",
    "\n",
    "def stopWordsRemoval(words):\n",
    "    words_list=[]\n",
    "    for w in words:\n",
    "        if w not in stopwords:\n",
    "            words_list.append(w)\n",
    "    return words_list\n",
    "\n",
    "def removeLinks(words):\n",
    "    words_list=[]\n",
    "    for w in words:\n",
    "        if not re.match('[www]',w):\n",
    "            words_list.append(w)\n",
    "    return words_list\n",
    "\n",
    "def spaceRemoval(words):\n",
    "    words_list=[]\n",
    "    for w in words:\n",
    "        if w!='':\n",
    "            words_list.append(w)\n",
    "    return words_list\n",
    "\n",
    "\n",
    "def dataExtraction(words):\n",
    "    words=nonAsciiChar(words)\n",
    "    words=spaceRemoval(words)\n",
    "    words=stopWordsRemoval(words)\n",
    "    words=stemWordsRemoval(words)\n",
    "    words=removeLinks(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentences(sentences_df):\n",
    "    sentences_df=sentences_df[sentences_df['sentences'].str.match('^[A-Z a-z 0-9]+')]\n",
    "    sentences_df=sentences_df.drop_duplicates(keep=False).reset_index(drop=True)\n",
    "    sentences_df['words'] = sentences_df.sentences.apply(word_tokenize)\n",
    "    sentences_df['words'] = sentences_df.words.apply(dataExtraction)\n",
    "    sentences_df['text'] = sentences_df.words.apply(lambda words: \" \".join(words))\n",
    "    sentences_df = sentences_df.drop(columns=[\"words\",\"sentences\"])\n",
    "    return sentences_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_words = ['amazon', 'amzn']\n",
    "amazon_news_df = sentences_df[sentences_df.sentences.str.contains(\"|\".join(amazon_words))]\n",
    "amazon_news_df = process_sentences(amazon_news_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_words = ['apple', 'aapl']\n",
    "apple_news_df = sentences_df[sentences_df.sentences.str.contains(\"|\".join(apple_words))]\n",
    "apple_news_df = process_sentences(apple_news_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "      <th>day</th>\n",
       "      <th>time</th>\n",
       "      <th>site</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>08:47:00</td>\n",
       "      <td>finance.yahoo.com</td>\n",
       "      <td>itun huge hit compani viewer migrat increasing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>01:29:00</td>\n",
       "      <td>www.financialnewsusa.com</td>\n",
       "      <td>scorecard facebook fb amazon amzn netflix nflx...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>01:29:00</td>\n",
       "      <td>www.financialnewsusa.com</td>\n",
       "      <td>amazon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>01:29:00</td>\n",
       "      <td>www.financialnewsusa.com</td>\n",
       "      <td>amazon share increas stock could soon present ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>01:29:00</td>\n",
       "      <td>www.financialnewsusa.com</td>\n",
       "      <td>fang stock news quot facebook amazon netflix g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label_1 label_2         day      time                      site  \\\n",
       "0     NaN     NaN  2018-01-01  08:47:00         finance.yahoo.com   \n",
       "1     NaN     NaN  2018-01-01  01:29:00  www.financialnewsusa.com   \n",
       "2     NaN     NaN  2018-01-01  01:29:00  www.financialnewsusa.com   \n",
       "3     NaN     NaN  2018-01-01  01:29:00  www.financialnewsusa.com   \n",
       "4     NaN     NaN  2018-01-01  01:29:00  www.financialnewsusa.com   \n",
       "\n",
       "                                                text  \n",
       "0  itun huge hit compani viewer migrat increasing...  \n",
       "1  scorecard facebook fb amazon amzn netflix nflx...  \n",
       "2                                             amazon  \n",
       "3  amazon share increas stock could soon present ...  \n",
       "4  fang stock news quot facebook amazon netflix g...  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(431887, 6)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apple_news_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_labels = amazon_news_df.drop(columns=['label_2'])\n",
    "amazon_labels = amazon_labels.rename(columns={\"label_1\":\"label\"})\n",
    "amazon_labels = amazon_labels.dropna(subset=['label'])\n",
    "amazon_labels.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_labels = apple_news_df.drop(columns=['label_1'])\n",
    "apple_labels = apple_labels.rename(columns={\"label_2\":\"label\"})\n",
    "apple_labels = apple_labels.dropna(subset=['label'])\n",
    "apple_labels.reset_index(drop=True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2018-01-01', '2018-01-03', '2018-01-06', '2018-01-04',\n",
       "       '2018-01-07', '2018-01-02', '2017-12-31', '2018-01-05',\n",
       "       '2018-01-08', '2018-01-15', '2018-01-14', '2018-01-13',\n",
       "       '2018-01-12', '2017-12-30', '2018-01-09', '2018-01-16',\n",
       "       '2017-12-22', '2018-01-10', '2018-01-18', '2018-01-17',\n",
       "       '2017-12-08', '2017-12-12', '2018-01-11', '2018-01-19',\n",
       "       '2018-01-20', '2018-01-21', '2018-01-22', '2018-01-23',\n",
       "       '2018-01-24', '2018-01-25', '2018-01-27', '2018-01-26',\n",
       "       '2018-01-28', '2018-01-29', '2018-01-30', '2018-02-01',\n",
       "       '2018-02-02', '2018-02-03', '2018-02-04', '2018-01-31',\n",
       "       '2018-02-06', '2018-02-05', '2018-02-10', '2018-02-08',\n",
       "       '2018-02-11', '2018-02-07', '2018-02-09', '2018-02-12',\n",
       "       '2018-02-14', '2018-02-13', '2018-02-15', '2018-02-17',\n",
       "       '2018-02-16', '2018-02-18', '2018-02-19', '2018-02-21',\n",
       "       '2018-02-20', '2018-02-22', '2018-02-23', '2018-02-24',\n",
       "       '2018-02-25', '2018-02-26', '2018-02-27', '2018-02-28',\n",
       "       '2018-03-03', '2018-03-02', '2018-03-04', '2018-03-05',\n",
       "       '2018-03-01', '2018-03-07', '2018-03-10', '2018-03-08',\n",
       "       '2018-03-09', '2018-03-06', '2018-03-11', '2018-03-12',\n",
       "       '2018-03-13', '2018-03-14', '2018-03-15', '2018-03-17',\n",
       "       '2018-03-18', '2018-03-19', '2018-03-20', '2018-03-16',\n",
       "       '2018-03-21', '2018-03-22', '2018-03-23', '2018-03-24',\n",
       "       '2018-03-25', '2018-03-26', '2018-03-27', '2018-03-28',\n",
       "       '2018-03-29', '2018-03-30', '2018-03-31', '2018-04-02',\n",
       "       '2018-04-01', '2018-04-03', '2018-04-05', '2018-04-06',\n",
       "       '2018-04-07', '2018-04-04', '2018-04-08', '2018-04-09',\n",
       "       '2018-04-11', '2018-04-15', '2018-04-14', '2018-04-12',\n",
       "       '2018-04-13', '2018-04-10', '2018-04-16', '2018-04-19',\n",
       "       '2018-04-20', '2018-04-21', '2018-04-18', '2018-04-17',\n",
       "       '2018-04-22', '2018-04-25', '2018-04-27', '2018-04-28',\n",
       "       '2018-04-24', '2018-04-23', '2018-04-26', '2018-04-29',\n",
       "       '2018-04-30', '2018-05-04', '2018-05-02', '2018-05-01',\n",
       "       '2018-05-03', '2018-05-05', '2018-05-06', '2018-05-07',\n",
       "       '2018-05-08', '2018-05-09', '2018-05-10', '2018-05-11',\n",
       "       '2018-05-12', '2018-05-13', '2018-05-15', '2018-05-14',\n",
       "       '2018-05-16', '2018-05-18', '2018-05-17', '2018-05-19',\n",
       "       '2018-05-20', '2018-05-21', '2018-05-23', '2018-05-22',\n",
       "       '2018-05-24', '2018-05-25', '2018-05-26', '2018-05-27',\n",
       "       '2018-05-28', '2018-05-29', '2018-05-30', '2018-05-31',\n",
       "       '2018-06-03', '2018-06-04', '2018-06-02', '2018-06-01',\n",
       "       '2018-06-06', '2018-06-05', '2018-06-07', '2018-06-09',\n",
       "       '2018-06-08', '2018-06-10', '2018-06-12', '2018-06-11',\n",
       "       '2018-06-14', '2018-06-16', '2018-06-15', '2018-06-17',\n",
       "       '2018-06-13', '2018-06-18', '2018-06-22', '2018-06-20',\n",
       "       '2018-06-23', '2018-06-24', '2018-06-21', '2018-06-19',\n",
       "       '2018-06-26', '2018-06-27', '2018-06-25', '2018-06-28',\n",
       "       '2018-06-29', '2018-06-30', '2018-07-01', '2018-07-04',\n",
       "       '2018-07-07', '2018-07-08', '2018-07-09', '2018-07-11',\n",
       "       '2018-07-10', '2018-07-03', '2018-07-05', '2018-07-06',\n",
       "       '2018-07-14', '2018-07-02', '2018-07-13', '2018-07-12',\n",
       "       '2018-07-15', '2018-07-17', '2018-07-18', '2018-07-19',\n",
       "       '2018-07-20', '2018-07-21', '2018-07-16', '2018-07-22',\n",
       "       '2018-07-23', '2018-07-24', '2018-07-25', '2018-07-28',\n",
       "       '2018-07-27', '2018-07-29', '2018-07-30', '2018-07-26',\n",
       "       '2018-07-31', '2018-08-02', '2018-08-01', '2018-08-03',\n",
       "       '2018-08-04', '2018-08-06', '2018-08-05', '2018-08-07',\n",
       "       '2018-08-09', '2018-08-08', '2018-08-10', '2018-08-11',\n",
       "       '2018-08-12', '2018-08-13', '2018-08-14', '2018-08-19',\n",
       "       '2018-08-18', '2018-08-22', '2018-08-16', '2018-08-17',\n",
       "       '2018-08-15', '2018-08-20', '2018-08-21', '2018-08-25',\n",
       "       '2018-08-26', '2018-08-30', '2018-08-24', '2018-08-23',\n",
       "       '2018-08-28', '2018-08-27', '2018-08-29', '2018-08-31',\n",
       "       '2018-09-01', '2018-09-03', '2018-09-02', '2018-09-08',\n",
       "       '2018-09-07', '2018-09-05', '2018-09-04', '2018-09-09',\n",
       "       '2018-09-06', '2018-09-10', '2018-09-11', '2018-09-13',\n",
       "       '2018-09-14', '2018-09-12', '2018-09-16', '2018-09-15',\n",
       "       '2018-09-17', '2018-09-22', '2018-09-19', '2018-09-18',\n",
       "       '2018-09-23', '2018-09-21', '2018-09-20', '2018-09-27',\n",
       "       '2018-09-26', '2018-09-25', '2018-09-24', '2018-09-28',\n",
       "       '2018-09-29', '2018-09-30', '2018-10-07', '2018-10-06',\n",
       "       '2018-10-01', '2018-10-04', '2018-10-02', '2018-10-03',\n",
       "       '2018-10-08', '2018-10-05', '2018-10-09', '2018-10-10',\n",
       "       '2018-10-12', '2018-10-14', '2018-10-13', '2018-10-11',\n",
       "       '2018-10-16', '2018-10-17', '2018-10-15', '2018-10-20',\n",
       "       '2018-10-18', '2018-10-21', '2018-10-22', '2018-10-25',\n",
       "       '2018-10-24', '2018-10-19', '2018-10-23', '2018-10-27',\n",
       "       '2018-10-26', '2018-10-28', '2018-10-29', '2018-10-30',\n",
       "       '2018-10-31', '2018-11-01', '2018-11-03', '2018-11-02',\n",
       "       '2018-11-04', '2018-11-07', '2018-11-05', '2018-11-09',\n",
       "       '2018-11-06', '2018-11-10', '2018-11-11', '2018-11-12',\n",
       "       '2018-11-08', '2018-11-14', '2018-11-13', '2018-11-15',\n",
       "       '2018-11-17', '2018-11-18', '2018-11-16', '2018-11-20',\n",
       "       '2018-11-19', '2018-11-22', '2018-11-21', '2018-11-27',\n",
       "       '2018-11-26', '2018-11-23', '2018-11-24', '2018-11-25',\n",
       "       '2018-11-29', '2018-11-28', '2018-11-30', '2018-12-01',\n",
       "       '2018-12-05', '2018-12-04', '2018-12-02', '2018-12-03',\n",
       "       '2018-12-06', '2018-12-07', '2018-12-08', '2018-12-09',\n",
       "       '2018-12-11', '2018-12-10', '2018-12-12', '2018-12-13',\n",
       "       '2018-12-15', '2018-12-14', '2018-12-16', '2018-12-17',\n",
       "       '2018-12-18', '2018-12-20', '2018-12-21', '2018-12-19',\n",
       "       '2018-12-22', '2018-12-24', '2018-12-23', '2018-12-25',\n",
       "       '2018-12-29', '2018-12-27', '2018-12-26', '2018-12-28',\n",
       "       '2018-12-30', '2018-12-31', '2019-01-01', '2019-01-02',\n",
       "       '2019-01-03', '2019-01-04', '2019-01-05', '2019-01-06',\n",
       "       '2019-01-07', '2019-01-09', '2019-01-08', '2019-01-12',\n",
       "       '2019-01-10', '2019-01-13', '2019-01-11', '2019-01-14',\n",
       "       '2019-01-15', '2019-01-16', '2019-01-18', '2019-01-19',\n",
       "       '2019-01-17', '2019-01-21', '2019-01-20', '2019-01-27',\n",
       "       '2019-01-26', '2019-01-22', '2019-01-28', '2019-01-24',\n",
       "       '2019-01-23', '2019-01-25', '2019-01-29', '2019-01-30',\n",
       "       '2019-01-31', '2019-02-02', '2019-02-03', '2019-02-01',\n",
       "       '2019-02-04', '2019-02-06', '2019-02-05', '2019-02-07'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apple_news_df[apple_news_df.label_2.isna()].day.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2018-01-01', '2018-01-03', '2018-01-06', '2018-01-04',\n",
       "       '2018-01-02', '2018-01-05', '2018-01-07', '2017-12-31',\n",
       "       '2018-01-08', '2018-01-15', '2018-01-14', '2018-01-13',\n",
       "       '2018-01-12', '2018-01-09', '2018-01-16', '2017-12-22',\n",
       "       '2017-12-30', '2018-01-10', '2018-01-17', '2018-01-21',\n",
       "       '2018-01-18', '2018-01-20', '2018-01-22', '2018-01-19',\n",
       "       '2018-01-23', '2018-01-27', '2018-01-26', '2018-01-25',\n",
       "       '2018-01-24', '2018-01-28', '2018-01-29', '2018-01-30',\n",
       "       '2018-02-01', '2018-02-03', '2018-02-02', '2018-02-04',\n",
       "       '2018-01-31', '2018-02-05', '2018-02-08', '2018-02-10',\n",
       "       '2018-02-09', '2018-02-11', '2018-02-06', '2018-02-12',\n",
       "       '2018-02-13', '2018-02-19', '2018-02-14', '2018-02-15',\n",
       "       '2018-02-18', '2018-02-17', '2018-02-23', '2018-02-16',\n",
       "       '2018-02-21', '2018-02-22', '2018-02-24', '2018-02-25',\n",
       "       '2018-02-26', '2018-02-27', '2018-02-20', '2018-02-28',\n",
       "       '2018-03-03', '2018-03-05', '2018-03-04', '2018-03-01',\n",
       "       '2018-03-02', '2018-03-07', '2018-03-08', '2018-03-11',\n",
       "       '2018-03-10', '2018-03-09', '2018-03-13', '2018-03-14',\n",
       "       '2018-03-12', '2018-03-06', '2018-03-15', '2018-03-17',\n",
       "       '2018-03-20', '2018-03-16', '2018-03-18', '2018-03-19',\n",
       "       '2018-03-22', '2018-03-24', '2018-03-25', '2018-03-26',\n",
       "       '2018-03-21', '2018-03-23', '2018-03-27', '2018-03-30',\n",
       "       '2018-03-31', '2018-03-29', '2018-03-28', '2018-04-02',\n",
       "       '2018-04-05', '2018-04-06', '2018-04-01', '2018-04-04',\n",
       "       '2018-04-03', '2018-04-08', '2018-04-09', '2018-04-07',\n",
       "       '2018-04-14', '2018-04-15', '2018-04-10', '2018-04-11',\n",
       "       '2018-04-12', '2018-04-20', '2018-04-18', '2018-04-16',\n",
       "       '2018-04-21', '2018-04-13', '2018-04-17', '2018-04-19',\n",
       "       '2018-04-22', '2018-04-27', '2018-04-24', '2018-04-23',\n",
       "       '2018-04-28', '2018-04-25', '2018-04-29', '2018-04-30',\n",
       "       '2018-04-26', '2018-05-02', '2018-05-01', '2018-05-03',\n",
       "       '2018-05-04', '2018-05-06', '2018-05-05', '2018-05-08',\n",
       "       '2018-05-07', '2018-05-09', '2018-05-11', '2018-05-10',\n",
       "       '2018-05-12', '2018-05-13', '2018-05-16', '2018-05-18',\n",
       "       '2018-05-14', '2018-05-15', '2018-05-17', '2018-05-19',\n",
       "       '2018-05-20', '2018-05-23', '2018-05-22', '2018-05-21',\n",
       "       '2018-05-28', '2018-05-27', '2018-05-26', '2018-05-24',\n",
       "       '2018-05-25', '2018-05-29', '2018-05-30', '2018-05-31',\n",
       "       '2018-06-03', '2018-06-01', '2018-06-02', '2018-06-06',\n",
       "       '2018-06-07', '2018-06-05', '2018-06-04', '2018-06-08',\n",
       "       '2018-06-09', '2018-06-10', '2018-06-12', '2018-06-11',\n",
       "       '2018-06-14', '2018-06-16', '2018-06-17', '2018-06-15',\n",
       "       '2018-06-13', '2018-06-20', '2018-06-24', '2018-06-18',\n",
       "       '2018-06-23', '2018-06-19', '2018-06-21', '2018-06-22',\n",
       "       '2018-06-25', '2018-06-27', '2018-06-29', '2018-06-30',\n",
       "       '2018-06-28', '2018-06-26', '2018-07-05', '2018-07-07',\n",
       "       '2018-07-08', '2018-07-01', '2018-07-06', '2018-07-11',\n",
       "       '2018-07-03', '2018-07-09', '2018-07-10', '2018-07-04',\n",
       "       '2018-07-02', '2018-07-14', '2018-07-15', '2018-07-17',\n",
       "       '2018-07-12', '2018-07-13', '2018-07-16', '2018-07-21',\n",
       "       '2018-07-22', '2018-07-20', '2018-07-19', '2018-07-18',\n",
       "       '2018-07-24', '2018-07-25', '2018-07-23', '2018-07-27',\n",
       "       '2018-07-29', '2018-07-28', '2018-07-30', '2018-07-26',\n",
       "       '2018-07-31', '2018-08-01', '2018-08-04', '2018-08-02',\n",
       "       '2018-08-03', '2018-08-05', '2018-08-07', '2018-08-09',\n",
       "       '2018-08-06', '2018-08-08', '2018-08-12', '2018-08-13',\n",
       "       '2018-08-10', '2018-08-18', '2018-08-19', '2018-08-16',\n",
       "       '2018-08-14', '2018-08-20', '2018-08-11', '2018-08-17',\n",
       "       '2018-08-15', '2018-08-21', '2018-08-22', '2018-08-30',\n",
       "       '2018-08-24', '2018-08-25', '2018-08-29', '2018-08-27',\n",
       "       '2018-08-26', '2018-08-23', '2018-08-28', '2018-09-01',\n",
       "       '2018-09-07', '2018-08-31', '2018-09-05', '2018-09-08',\n",
       "       '2018-09-06', '2018-09-03', '2018-09-04', '2018-09-02',\n",
       "       '2018-09-09', '2018-09-11', '2018-09-13', '2018-09-12',\n",
       "       '2018-09-14', '2018-09-10', '2018-09-15', '2018-09-19',\n",
       "       '2018-09-22', '2018-09-17', '2018-09-18', '2018-09-16',\n",
       "       '2018-09-23', '2018-09-21', '2018-09-27', '2018-09-24',\n",
       "       '2018-09-20', '2018-09-25', '2018-09-28', '2018-09-29',\n",
       "       '2018-09-30', '2018-09-26', '2018-10-07', '2018-10-04',\n",
       "       '2018-10-03', '2018-10-01', '2018-10-06', '2018-10-02',\n",
       "       '2018-10-08', '2018-10-09', '2018-10-05', '2018-10-10',\n",
       "       '2018-10-14', '2018-10-13', '2018-10-12', '2018-10-11',\n",
       "       '2018-10-17', '2018-10-16', '2018-10-20', '2018-10-25',\n",
       "       '2018-10-15', '2018-10-19', '2018-10-21', '2018-10-27',\n",
       "       '2018-10-26', '2018-10-22', '2018-10-23', '2018-10-24',\n",
       "       '2018-10-18', '2018-10-28', '2018-10-29', '2018-10-31',\n",
       "       '2018-11-01', '2018-11-03', '2018-11-04', '2018-11-02',\n",
       "       '2018-11-05', '2018-11-06', '2018-11-10', '2018-11-07',\n",
       "       '2018-11-11', '2018-11-09', '2018-11-12', '2018-11-08',\n",
       "       '2018-11-13', '2018-11-14', '2018-11-17', '2018-11-18',\n",
       "       '2018-11-15', '2018-11-16', '2018-11-20', '2018-11-22',\n",
       "       '2018-11-21', '2018-11-26', '2018-11-19', '2018-10-30',\n",
       "       '2018-11-23', '2018-11-24', '2018-11-27', '2018-11-25',\n",
       "       '2018-11-28', '2018-11-30', '2018-11-29', '2018-12-05',\n",
       "       '2018-12-04', '2018-12-01', '2018-12-02', '2018-12-06',\n",
       "       '2018-12-03', '2018-12-07', '2018-12-08', '2018-12-09',\n",
       "       '2018-12-10', '2018-12-11', '2018-12-12', '2018-12-15',\n",
       "       '2018-12-16', '2018-12-14', '2018-12-17', '2018-12-18',\n",
       "       '2018-12-13', '2018-12-20', '2018-12-21', '2018-12-19',\n",
       "       '2018-12-22', '2018-12-23', '2018-12-25', '2018-12-24',\n",
       "       '2018-12-26', '2018-12-27', '2018-12-30', '2018-12-29',\n",
       "       '2018-12-28', '2018-12-31', '2019-01-02', '2019-01-01',\n",
       "       '2019-01-03', '2019-01-05', '2019-01-06', '2019-01-07',\n",
       "       '2019-01-04', '2019-01-08', '2019-01-12', '2019-01-09',\n",
       "       '2019-01-10', '2019-01-13', '2019-01-14', '2019-01-11',\n",
       "       '2019-01-15', '2019-01-16', '2019-01-18', '2019-01-19',\n",
       "       '2019-01-17', '2019-01-21', '2019-01-20', '2019-01-26',\n",
       "       '2019-01-27', '2019-01-28', '2019-01-25', '2019-01-24',\n",
       "       '2019-01-22', '2019-01-29', '2019-01-23', '2019-01-30',\n",
       "       '2019-01-31', '2019-02-03', '2019-02-02', '2019-02-04',\n",
       "       '2019-02-06', '2019-02-05', '2019-02-01', '2019-02-07'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_news_df[amazon_news_df.label_1.isna()].day.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_labels.to_csv(\"../data/processed/full/apple_labelled_60_special.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_labels.to_csv(\"../data/processed/full/amazon_labelled_60_special.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2017-03-13'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_stock_price_60.iloc[1].day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date_time                        2017-12-21T14:27:00.000+02:00\n",
       "text         there are still plenty of stocks that fit the ...\n",
       "day                                                 2017-12-21\n",
       "time                                                  12:27:00\n",
       "site                                         www.thestreet.com\n",
       "Name: 485, dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.iloc[18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(298773, 5)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apple_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
