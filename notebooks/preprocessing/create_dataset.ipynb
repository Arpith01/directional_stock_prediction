{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pytz\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def readData(news_dir):\n",
    "    data_text = []\n",
    "    data_published = []\n",
    "    date = []\n",
    "    time = []\n",
    "    site = []\n",
    "    for folder in os.scandir(news_dir):\n",
    "        print(os.scandir(news_dir),type(os.scandir(news_dir)))\n",
    "        if folder.name!=\".DS_Store\":\n",
    "            for entry in os.scandir(news_dir+folder.name):\n",
    "                data = json.load(open(news_dir+folder.name+\"/\"+entry.name, encoding='utf-8'))\n",
    "                if data['published']!=\"\":\n",
    "                    data_published.append(data['published'])\n",
    "                    date_time = datetime.datetime.strptime(data['published'], \"%Y-%m-%dT%H:%M:%S.%f%z\").astimezone(pytz.utc)\n",
    "                    date.append(str(date_time.date()))\n",
    "                    time.append(str(date_time.time()))\n",
    "                else:\n",
    "                    data_published.append(\"No Value\")\n",
    "                    date.append(\"No Value\")\n",
    "                    time.append(\"No Value\")\n",
    "                if data['text']!=\"\":\n",
    "                    data_text.append(data['text'].lower())\n",
    "                else:\n",
    "                    data_text.append(\"no value\")\n",
    "                    \n",
    "                if data['thread']['site_full'] != '':\n",
    "                    site.append(data['thread']['site_full'])\n",
    "                else:\n",
    "                    site.append(\"unknown\")\n",
    "    news_df=pd.DataFrame({'date_time':data_published, 'text':data_text, 'day':date, 'time':time, 'site':site})\n",
    "    return news_df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "news_directory = \"../../Data/News/\"\n",
    "\n",
    "news_df = readData(news_directory)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "news_df.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "news_df.sort_values([\"day\", \"time\"],axis = 0, ascending=[True, True], inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "news_df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sites = news_df.site.unique()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sorted(sites)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def get_sentences(paragraph):\n",
    "    sentences = []\n",
    "    \n",
    "    first_split = sent_tokenize(paragraph)\n",
    "    \n",
    "    for maybe_sentences in first_split:\n",
    "        our_sentences = maybe_sentences.split(\"\\n\")\n",
    "        sentences.extend(our_sentences)\n",
    "        \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "news_df['sentences'] = news_df.text.apply(get_sentences)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "news_df['n_sentences'] = news_df.sentences.apply(lambda s_list: len(s_list))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "news_df.n_sentences.describe()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sentences_df = news_df.drop(columns=[\"n_sentences\",\"text\"]).explode('sentences').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sentences_df = sentences_df.dropna(subset=['sentences'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "amazon_words = ['amazon', 'amzn']\n",
    "amazon_news_df = sentences_df[sentences_df.sentences.str.contains(\"|\".join(amazon_words))]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "apple_words = ['apple', 'aapl']\n",
    "apple_news_df = sentences_df[sentences_df.sentences.str.contains(\"|\".join(apple_words))]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import unicodedata\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "from nltk.stem import LancasterStemmer,PorterStemmer\n",
    "\n",
    "\n",
    "def nonAsciiChar(words):\n",
    "    words_list=[]\n",
    "    for w in words:\n",
    "        w=re.sub('[^a-zA-Z]+','',re.sub(r'[\\W\\d]','',w.lower()))\n",
    "        format_words=unicodedata.normalize('NFKD', w).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        words_list.append(format_words)\n",
    "        \n",
    "    return words_list\n",
    "def stemWordsRemoval(words):\n",
    "    stemmer = PorterStemmer()\n",
    "    words_list=[]\n",
    "    for word in words:\n",
    "        word=stemmer.stem(word)\n",
    "        if word not in words_list:\n",
    "            words_list.append(word)\n",
    "    return words_list\n",
    "\n",
    "def stopWordsRemoval(words):\n",
    "    words_list=[]\n",
    "    for w in words:\n",
    "        if w not in stopwords:\n",
    "            words_list.append(w)\n",
    "    return words_list\n",
    "\n",
    "def removeLinks(words):\n",
    "    words_list=[]\n",
    "    for w in words:\n",
    "        if not re.match('[www]',w):\n",
    "            words_list.append(w)\n",
    "    return words_list\n",
    "\n",
    "def spaceRemoval(words):\n",
    "    words_list=[]\n",
    "    for w in words:\n",
    "        if w!='':\n",
    "            words_list.append(w)\n",
    "    return words_list\n",
    "\n",
    "\n",
    "def dataExtraction(words):\n",
    "    words=nonAsciiChar(words)\n",
    "    words=spaceRemoval(words)\n",
    "    words=stopWordsRemoval(words)\n",
    "    words=stemWordsRemoval(words)\n",
    "    words=removeLinks(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def process_sentences(sentences_df):\n",
    "    sentences_df=sentences_df[sentences_df['sentences'].str.match('^[A-Z a-z 0-9]+')]\n",
    "    sentences_df=sentences_df.drop_duplicates(keep=False).reset_index(drop=True)\n",
    "    sentences_df['words'] = sentences_df.sentences.apply(word_tokenize)\n",
    "    sentences_df['words'] = sentences_df.words.apply(dataExtraction)\n",
    "    sentences_df['text'] = sentences_df.words.apply(lambda words: \" \".join(words))\n",
    "    sentences_df = sentences_df.drop(columns=[\"words\",\"sentences\"])\n",
    "    return sentences_df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "amazon_news_df = process_sentences(amazon_news_df)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "apple_news_df = process_sentences(apple_news_df)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def load_stock_price_dataset(path):\n",
    "    stock_df=pd.read_csv(path,names=['day','time','x1','price','x2','x3','x4'])[['day','time','price']]\n",
    "    stock_df.day = stock_df.day.apply(lambda s:s.replace('.','-'))\n",
    "    return stock_df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "amazon_stock_price_60 = load_stock_price_dataset('../../Data/CHARTS/AMAZON60.csv')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "apple_stock_price_60 = load_stock_price_dataset('../../Data/CHARTS/APPLE60.csv')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def assign_labels(news_df, stocks_df):\n",
    "    labels_df = pd.DataFrame(columns=[\"text\", \"label\", \"day\", \"time\", \"site\"])\n",
    "    for row_index in tqdm(range(len(news_df))):\n",
    "        row = news_df.iloc[row_index]\n",
    "        \n",
    "        day = row.day\n",
    "        time = row.time\n",
    "        text = row.text\n",
    "        \n",
    "#         print(day, time)\n",
    "        next_time_step_indexes = stocks_df[(stocks_df.day == day) & (stocks_df.time > time)].index\n",
    "        if len(next_time_step_indexes) != 0:\n",
    "            next_time_step_index = next_time_step_indexes[0]\n",
    "#             print(\"pass\")\n",
    "        else:\n",
    "            next_time_step_indexes = stocks_df[(stocks_df.day > day)].index\n",
    "            if len(next_time_step_indexes) != 0:\n",
    "                next_time_step_index = next_time_step_indexes[0]\n",
    "#                 print(\"pass\")\n",
    "            else:\n",
    "#                 print(\"fail\")    \n",
    "                continue\n",
    "        prev_time_step_index = next_time_step_index - 1 if next_time_step_index>0 else 0\n",
    "        \n",
    "        label = 1 if stocks_df.iloc[next_time_step_index].price >= stocks_df.iloc[prev_time_step_index].price else 0\n",
    "        labels_df.loc[len(labels_df)] = [text, label, day, time, row.site]\n",
    "        \n",
    "        \n",
    "    return labels_df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "amazon_labels = assign_labels(amazon_news_df, amazon_stock_price_60)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "apple_labels = assign_labels(apple_news_df, apple_stock_price_60)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "amazon_news_df.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "apple_news_df.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "amazon_labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fresh start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pytz\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "import unicodedata\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "from nltk.stem import LancasterStemmer,PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(news_dir):\n",
    "    data_text = []\n",
    "    data_published = []\n",
    "    date = []\n",
    "    time = []\n",
    "    site = []\n",
    "    for folder in os.scandir(news_dir):\n",
    "        print(os.scandir(news_dir),type(os.scandir(news_dir)))\n",
    "        if folder.name!=\".DS_Store\":\n",
    "            for entry in os.scandir(news_dir+folder.name):\n",
    "                data = json.load(open(news_dir+folder.name+\"/\"+entry.name, encoding='utf-8'))\n",
    "                if data['published']!=\"\":\n",
    "                    data_published.append(data['published'])\n",
    "                    date_time = datetime.datetime.strptime(data['published'], \"%Y-%m-%dT%H:%M:%S.%f%z\").astimezone(pytz.utc)\n",
    "                    date.append(str(date_time.date()))\n",
    "                    time.append(str(date_time.time()))\n",
    "                else:\n",
    "                    data_published.append(\"No Value\")\n",
    "                    date.append(\"No Value\")\n",
    "                    time.append(\"No Value\")\n",
    "                if data['text']!=\"\":\n",
    "                    data_text.append(data['text'].lower())\n",
    "                else:\n",
    "                    data_text.append(\"no value\")\n",
    "                    \n",
    "                if data['thread']['site_full'] != '':\n",
    "                    site.append(data['thread']['site_full'])\n",
    "                else:\n",
    "                    site.append(\"unknown\")\n",
    "    news_df=pd.DataFrame({'date_time':data_published, 'text':data_text, 'day':date, 'time':time, 'site':site})\n",
    "    return news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_directory = \"../../Data/News/\"\n",
    "\n",
    "news_df = readData(news_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.sort_values([\"day\", \"time\"],axis = 0, ascending=[True, True], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stock_price_dataset(path):\n",
    "    stock_df=pd.read_csv(path,names=['day','time','x1','price','x2','x3','x4'])[['day','time','price']]\n",
    "    stock_df.day = stock_df.day.apply(lambda s:s.replace('.','-'))\n",
    "    return stock_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_stock_price_60 = load_stock_price_dataset('../../Data/CHARTS/AMAZON60.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_stock_price_60 = load_stock_price_dataset('../../Data/CHARTS/APPLE60.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(stocks_df, day, time):\n",
    "    next_time_step_indexes = stocks_df[(stocks_df.day == day) & (stocks_df.time > time)].index\n",
    "    if len(next_time_step_indexes) != 0:\n",
    "        next_time_step_index = next_time_step_indexes[0]\n",
    "    #    print(\"pass\")\n",
    "    else:\n",
    "#         return np.nan\n",
    "        next_time_step_indexes = stocks_df[(stocks_df.day > day)].index\n",
    "        if len(next_time_step_indexes) != 0:\n",
    "            next_time_step_index = next_time_step_indexes[0]\n",
    "    #       print(\"pass\")\n",
    "        else:\n",
    "    #       print(\"fail\")    \n",
    "            return np.nan\n",
    "    prev_time_step_index = next_time_step_index - 1 if next_time_step_index>0 else 0\n",
    "    \n",
    "    label = 1 if stocks_df.iloc[next_time_step_index].price >= stocks_df.iloc[prev_time_step_index].price else 0\n",
    "    return label\n",
    "\n",
    "\n",
    "def assign_labels(news_df, stocks_df_1, stocks_df_2):\n",
    "    labels_df = pd.DataFrame(columns=[\"text\", \"label_1\", \"label_2\", \"day\", \"time\", \"site\"])\n",
    "    for row_index in tqdm(range(len(news_df))):\n",
    "        row = news_df.iloc[row_index]\n",
    "        \n",
    "        day = row.day\n",
    "        time = row.time\n",
    "        text = row.text\n",
    "        \n",
    "        label_1 = get_label(stocks_df_1, day, time)\n",
    "        label_2 = get_label(stocks_df_2, day, time)\n",
    "        \n",
    "        labels_df.loc[len(labels_df)] = [text, label_1, label_2, day, time, row.site]\n",
    "        \n",
    "        \n",
    "    return labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df = assign_labels(news_df, amazon_stock_price_60, apple_stock_price_60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(paragraph):\n",
    "    sentences = []\n",
    "    \n",
    "    first_split = sent_tokenize(paragraph)\n",
    "    \n",
    "    for maybe_sentences in first_split:\n",
    "        our_sentences = maybe_sentences.split(\"\\n\")\n",
    "        sentences.extend(our_sentences)\n",
    "        \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df['sentences'] = labels_df.text.apply(get_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df['n_sentences'] = labels_df.sentences.apply(lambda s_list: len(s_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df.n_sentences.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_df = labels_df.drop(columns=[\"n_sentences\",\"text\"]).explode('sentences').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_df = sentences_df.dropna(subset=['sentences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonAsciiChar(words):\n",
    "    words_list=[]\n",
    "    for w in words:\n",
    "        w=re.sub('[^a-zA-Z]+','',re.sub(r'[\\W\\d]','',w.lower()))\n",
    "        format_words=unicodedata.normalize('NFKD', w).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        words_list.append(format_words)\n",
    "        \n",
    "    return words_list\n",
    "def stemWordsRemoval(words):\n",
    "    stemmer = PorterStemmer()\n",
    "    words_list=[]\n",
    "    for word in words:\n",
    "        word=stemmer.stem(word)\n",
    "        if word not in words_list:\n",
    "            words_list.append(word)\n",
    "    return words_list\n",
    "\n",
    "def stopWordsRemoval(words):\n",
    "    words_list=[]\n",
    "    for w in words:\n",
    "        if w not in stopwords:\n",
    "            words_list.append(w)\n",
    "    return words_list\n",
    "\n",
    "def removeLinks(words):\n",
    "    words_list=[]\n",
    "    for w in words:\n",
    "        if not re.match('[www]',w):\n",
    "            words_list.append(w)\n",
    "    return words_list\n",
    "\n",
    "def spaceRemoval(words):\n",
    "    words_list=[]\n",
    "    for w in words:\n",
    "        if w!='':\n",
    "            words_list.append(w)\n",
    "    return words_list\n",
    "\n",
    "\n",
    "def dataExtraction(words):\n",
    "    words=nonAsciiChar(words)\n",
    "    words=spaceRemoval(words)\n",
    "    words=stopWordsRemoval(words)\n",
    "    words=stemWordsRemoval(words)\n",
    "    words=removeLinks(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentences(sentences_df):\n",
    "    sentences_df=sentences_df[sentences_df['sentences'].str.match('^[A-Z a-z 0-9]+')]\n",
    "    sentences_df=sentences_df.drop_duplicates(keep=False).reset_index(drop=True)\n",
    "    sentences_df['words'] = sentences_df.sentences.apply(word_tokenize)\n",
    "    sentences_df['words'] = sentences_df.words.apply(dataExtraction)\n",
    "    sentences_df['text'] = sentences_df.words.apply(lambda words: \" \".join(words))\n",
    "    sentences_df = sentences_df.drop(columns=[\"words\",\"sentences\"])\n",
    "    return sentences_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_words = ['amazon', 'amzn']\n",
    "amazon_news_df = sentences_df[sentences_df.sentences.str.contains(\"|\".join(amazon_words))]\n",
    "amazon_news_df = process_sentences(amazon_news_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_words = ['apple', 'aapl']\n",
    "apple_news_df = sentences_df[sentences_df.sentences.str.contains(\"|\".join(apple_words))]\n",
    "apple_news_df = process_sentences(apple_news_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_news_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_labels = amazon_news_df.drop(columns=['label_2'])\n",
    "amazon_labels = amazon_labels.rename(columns={\"label_1\":\"label\"})\n",
    "amazon_labels = amazon_labels.dropna(subset=['label'])\n",
    "amazon_labels.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_labels = apple_news_df.drop(columns=['label_1'])\n",
    "apple_labels = apple_labels.rename(columns={\"label_2\":\"label\"})\n",
    "apple_labels = apple_labels.dropna(subset=['label'])\n",
    "apple_labels.reset_index(drop=True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_news_df[apple_news_df.label_2.isna()].day.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_news_df[amazon_news_df.label_1.isna()].day.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_labels.to_csv(\"../data/processed/full/apple_labelled_60_special.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_labels.to_csv(\"../data/processed/full/amazon_labelled_60_special.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_stock_price_60.iloc[1].day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.iloc[18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
