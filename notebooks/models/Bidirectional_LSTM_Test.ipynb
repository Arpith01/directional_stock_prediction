{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import gensim\n",
    "import joblib\n",
    "from keras.models import load_model\n",
    "import gzip\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file='/Users/M NILESH/Downloads/Label10000.csv'\n",
    "df=pd.read_csv(input_file)\n",
    "df=df[['text']]\n",
    "df['word_count']=df['text'].apply(lambda x: len(str(x).split(\" \")))\n",
    "df.word_count.describe()\n",
    "common_words=pd.Series(''.join(df['text']).split()).value_counts()\n",
    "most_common=common_words[:3]\n",
    "least_common=common_words[-1000:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=[]\n",
    "labels=[]\n",
    "def removeWords(words):\n",
    "    final_words=[]\n",
    "    for word in words:\n",
    "        if not (word in most_common or word in least_common):\n",
    "            final_words.append(word)\n",
    "    return final_words\n",
    "for i in range(len(df)):\n",
    "    if i%10000==0:\n",
    "        print(\"------>\",i)\n",
    "    words=word_tokenize(df.loc[i].text)\n",
    "    # print(df.loc[i].text)\n",
    "    words=removeWords(words)\n",
    "    extracted_sentence=' '.join(word for word in words)\n",
    "    # print(df.loc[i].text)\n",
    "    texts.append(extracted_sentence)\n",
    "new_df=pd.DataFrame({\n",
    "    'text':texts\n",
    "})\n",
    "def tokenize(x):\n",
    "    x_tk = Tokenizer()\n",
    "    x_tk.fit_on_texts(x)\n",
    "    return x_tk.texts_to_sequences(x), x_tk\n",
    "def pad(x, length=None):\n",
    "    if length is None:\n",
    "        length = len(max(x, key=len))\n",
    "\n",
    "    return pad_sequences(x, maxlen=length, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_text=[]\n",
    "news_label=[]\n",
    "for n_z in new_df['text']:\n",
    "  tokenize_text.append(n_z)\n",
    "text_tokenized, text_tokenizer=tokenize(tokenize_text)\n",
    "text_padded=pad(text_tokenized)\n",
    "trainX=text_padded\n",
    "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "refined_out = trainX.astype(np.float)\n",
    "model = load_model('Bidirectional.h5')\n",
    "output=model.predict(refined_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newout=[]\n",
    "for i in range(len(output)):\n",
    "    if output[i][1]>0.5:\n",
    "        newout.append(1)\n",
    "    else:\n",
    "        newout.append(0)\n",
    "count1=0\n",
    "for i in range(len(newout)):\n",
    "    if newout[i]==y_test[i][1]:\n",
    "        count1+=1\n",
    "if (count1>len(newout)-count1):\n",
    "    print('1')\n",
    "else:\n",
    "    print('0')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
