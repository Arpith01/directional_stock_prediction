{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NB_API.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arpith01/directional_stock_prediction/blob/master/notebooks/models/NB_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GLjPuAR4fji"
      },
      "source": [
        "!pip install colabcode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcvF3aVK4jg0"
      },
      "source": [
        "!pip install fastapi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RPU3_n24pK3",
        "outputId": "90fa2c79-0252-490a-a5c3-0f053a86b250"
      },
      "source": [
        "import unicodedata\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.downloader.download('vader_lexicon')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import LancasterStemmer,PorterStemmer\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from tqdm import tqdm\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "from nltk.stem import LancasterStemmer,PorterStemmer\n",
        "\n",
        "\n",
        "def nonAsciiChar(words):\n",
        "    words_list=[]\n",
        "    for w in words:\n",
        "        w=re.sub('[^a-zA-Z]+','',re.sub(r'[\\W\\d]','',w.lower()))\n",
        "        format_words=unicodedata.normalize('NFKD', w).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "        words_list.append(format_words)\n",
        "        \n",
        "    return words_list\n",
        "def stemWordsRemoval(words):\n",
        "    stemmer = PorterStemmer()\n",
        "    words_list=[]\n",
        "    for word in words:\n",
        "        word=stemmer.stem(word)\n",
        "        if word not in words_list:\n",
        "            words_list.append(word)\n",
        "    return words_list\n",
        "\n",
        "def stopWordsRemoval(words):\n",
        "    words_list=[]\n",
        "    for w in words:\n",
        "        if w not in stopwords:\n",
        "            words_list.append(w)\n",
        "    return words_list\n",
        "\n",
        "def removeLinks(words):\n",
        "    words_list=[]\n",
        "    for w in words:\n",
        "        if not re.match('[www]',w):\n",
        "            words_list.append(w)\n",
        "    return words_list\n",
        "\n",
        "def spaceRemoval(words):\n",
        "    words_list=[]\n",
        "    for w in words:\n",
        "        if w!='':\n",
        "            words_list.append(w)\n",
        "    return words_list\n",
        "\n",
        "\n",
        "def dataExtraction(words):\n",
        "    words=nonAsciiChar(words)\n",
        "    words=spaceRemoval(words)\n",
        "    words=stopWordsRemoval(words)\n",
        "    words=stemWordsRemoval(words)\n",
        "    words=removeLinks(words)\n",
        "    return words"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opwDmjVQ4yZc"
      },
      "source": [
        "def get_sentences(news_article):\n",
        "    paragraphs = nltk.sent_tokenize(news_article.lower())\n",
        "    sentences=[]\n",
        "    is_amazon = []\n",
        "    amzn_sentences = []\n",
        "    apple_sentences = []\n",
        "    key_words=['amazon','apple','aapl','amzn']\n",
        "    amazon_keywords = ['amazon', 'amzn']\n",
        "    apple_keywords = ['apple', 'aapl']\n",
        "\n",
        "    for para in paragraphs:\n",
        "      sentences_list = para.split(\"\\n\")\n",
        "      for sentence in sentences_list:\n",
        "        if any(key in sentence for key in amazon_keywords):\n",
        "          sentences.append(sentence)\n",
        "          is_amazon.append(1)\n",
        "        if any(key in sentence for key in apple_keywords):\n",
        "          sentences.append(sentence)\n",
        "          is_amazon.append(0)\n",
        "          # if any(key in sentence for key in key_words):\n",
        "          #   sentences.append(sentence)\n",
        "\n",
        "\n",
        "    if len(sentences) == 0:\n",
        "      return None\n",
        "    \n",
        "    sentences_df = pd.DataFrame({'text':sentences, 'is_amazon':is_amazon})\n",
        "\n",
        "    sentences_df=sentences_df[sentences_df['text'].str.match('^[A-Z a-z 0-9]+')]\n",
        "\n",
        "    sentences_df=sentences_df.drop_duplicates(keep=False).reset_index(drop=True)\n",
        "\n",
        "    sentences_df['words'] = sentences_df.text.apply(word_tokenize)\n",
        "    sentences_df['words'] = sentences_df.words.apply(dataExtraction)\n",
        "    sentences_df['text'] = sentences_df.words.apply(lambda words: \" \".join(words))\n",
        "    sentences_df = sentences_df.drop(columns=\"words\")\n",
        "\n",
        "    return sentences_df"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfTZUVKe47HO"
      },
      "source": [
        "from pydantic import BaseModel\n",
        "from fastapi import FastAPI"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27ozejKW495A"
      },
      "source": [
        "from fastapi.middleware.cors import CORSMiddleware"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7r_rD8t5Aav"
      },
      "source": [
        "class Sentence(BaseModel):\n",
        "  news_article:str"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7UY81Zp5CY6"
      },
      "source": [
        "class Prediction(BaseModel):\n",
        "  prediction_apple:str\n",
        "  confidence_apple:str\n",
        "  prediction_amazon:str\n",
        "  confidence_amazon:str"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiVwdgpb5EG5"
      },
      "source": [
        "app = FastAPI()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pf5379tT5HgQ"
      },
      "source": [
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWKMuHGf825b",
        "outputId": "f58d3bc1-a05f-4aa2-b834-dc9c102b8520"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZp5X5HoMZD6"
      },
      "source": [
        "##Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVJ0s3R780iE",
        "outputId": "18a67adb-e8a5-443b-c969-5a6130a497fc"
      },
      "source": [
        "import pickle\n",
        "path = \"/content/drive/MyDrive/new_data_ARS/naive_bayes.sav\"\n",
        "infile = open(path,'rb')\n",
        "Naive_Bayes = pickle.load(infile)\n",
        "infile.close()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator GaussianNB from version 0.23.2 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
            "  UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inCiVPcV5RKi",
        "outputId": "0b7b31c7-7d10-410c-ac49-64dcd0f67075"
      },
      "source": [
        "saved_model = Naive_Bayes\n",
        "saved_model"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GaussianNB(priors=None, var_smoothing=1e-09)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blxNHPhBJZC3"
      },
      "source": [
        "path = \"/content/drive/MyDrive/new_data_ARS/vectorizer\"\n",
        "infile = open(path,'rb')\n",
        "vectorizer_dict = pickle.load(infile)\n",
        "infile.close()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhqy-CpOKjtX"
      },
      "source": [
        "vectorizer = vectorizer_dict['vectorizer']"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3rf5W7gKuUq",
        "outputId": "287bfcdf-95e0-4bf3-ab1c-db705afbc040"
      },
      "source": [
        "vectorizer"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
              "                lowercase=True, max_df=1.0, max_features=4000, min_df=10,\n",
              "                ngram_range=(2, 2), preprocessor=None, stop_words=None,\n",
              "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                tokenizer=None, vocabulary=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4OnyV2KCEdT"
      },
      "source": [
        "def get_features(X_test):\n",
        "  cls = SentimentIntensityAnalyzer()\n",
        "  Positive_X_test_text = []\n",
        "  Negative_X_test_text = []\n",
        "\n",
        "  for i in tqdm(X_test):\n",
        "      n1= cls.polarity_scores(i)[\"neg\"]\n",
        "      n2= cls.polarity_scores(i)[\"pos\"]\n",
        "      Positive_X_test_text.append(n1)\n",
        "      Negative_X_test_text.append(n2)\n",
        "  \n",
        "  # vectorizer = CountVectorizer(ngram_range = (2,2),max_features = 4000, min_df = 10)\n",
        "  \n",
        "  X_test_2gram_features = vectorizer.transform(X_test)\n",
        "  Sentiment_X_test_text = np.column_stack((Positive_X_test_text, Negative_X_test_text))\n",
        "\n",
        "  X_test_2gram_features_array = X_test_2gram_features.toarray()\n",
        "  Xtest = np.concatenate((Sentiment_X_test_text,X_test_2gram_features_array), axis = 1)\n",
        "  return Xtest"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaaLKe-55Jb9"
      },
      "source": [
        "@app.on_event(\"startup\")\n",
        "def load_model_from_file():\n",
        "  global inference_model\n",
        "  inference_model = saved_model\n",
        "\n",
        "\n",
        "@app.get('/')\n",
        "def index():\n",
        "  return{'message':'Post an article to this API to get inference'}\n",
        "\n",
        "@app.post('/predict')\n",
        "def classify_sentence(data:Sentence):\n",
        "  received_data = data.dict()\n",
        "  news_article = received_data['news_article']\n",
        "  sentences = get_sentences(news_article)\n",
        "  prediction_class_amzn = \"-1\"\n",
        "  prediction_class_apple = \"-1\"\n",
        "  confidence_amzn = '0'\n",
        "  confidence_apple = '0'\n",
        "\n",
        "  if sentences is None:\n",
        "    p = Prediction(prediction_apple=prediction_class_apple, confidence_apple=confidence_apple, prediction_amazon = prediction_class_amzn, confidence_amazon=confidence_amzn)\n",
        "    return p\n",
        "\n",
        "\n",
        "  amazon_sentences = sentences[sentences['is_amazon'] == 1].text\n",
        "  apple_sentences = sentences[sentences['is_amazon'] == 0].text\n",
        "\n",
        "  # posterior_prob = tf.sigmoid(inference_model.predict(sentences))\n",
        "\n",
        "  if(len(amazon_sentences) != 0):\n",
        "    amazon_sentences_test = get_features(amazon_sentences)\n",
        "    posterior_prob_amzn = inference_model.predict(amazon_sentences_test)\n",
        "    prediction_class_amzn = np.where(posterior_prob_amzn>0.5, 1, 0)\n",
        "    bin_count_amzn = np.bincount(prediction_class_amzn.flatten())\n",
        "\n",
        "    prediction_class_amzn = np.bincount(prediction_class_amzn.flatten()).argmax()\n",
        "\n",
        "    confidence_amzn = bin_count_amzn[prediction_class_amzn]/np.sum(bin_count_amzn)\n",
        "    confidence_amzn = str(confidence_amzn)\n",
        "    prediction_class_amzn = str(prediction_class_amzn)\n",
        "\n",
        "\n",
        "  if(len(apple_sentences) != 0):\n",
        "    apple_sentences_test = get_features(apple_sentences)\n",
        "    posterior_prob_apple = inference_model.predict(apple_sentences_test)\n",
        "    prediction_class_apple = np.where(posterior_prob_apple>0.5, 1, 0)\n",
        "    bin_count_apple = np.bincount(prediction_class_apple.flatten())\n",
        "\n",
        "    prediction_class_apple = np.bincount(prediction_class_apple.flatten()).argmax()\n",
        "\n",
        "    confidence_apple = bin_count_apple[prediction_class_apple]/np.sum(bin_count_apple)\n",
        "    prediction_class_apple = str(prediction_class_apple)\n",
        "    confidence_apple = str(confidence_apple)\n",
        "\n",
        "  # prediction_classes = np.where(posterior_prob>0.5, 1, 0)\n",
        "\n",
        "  # print(prediction_classes)\n",
        "  # prediction_class = np.bincount(prediction_classes.flatten()).argmax()\n",
        "  # p = Prediction(prediction=str(prediction_class))\n",
        "  p = Prediction(prediction_apple=prediction_class_apple, confidence_apple=confidence_apple, prediction_amazon = prediction_class_amzn, confidence_amazon=confidence_amzn)\n",
        "\n",
        "  return p"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoeADpmotKSX"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "amazon_full = pd.read_csv(\"/content/drive/MyDrive/Amazon_csvs_published_source/Label46623.csv\")\n",
        "\n",
        "apple_full = pd.read_csv(\"/content/drive/MyDrive/apple_csvs_published_source/Label30000.csv\")\n",
        "\n",
        "amazon_full = amazon_full[amazon_full.text.notnull()]\n",
        "apple_full = apple_full[apple_full.text.notnull()]\n",
        "\n",
        "amazon_test = amazon_full.sample(frac=0.5, random_state=42).reset_index(drop=True)\n",
        "apple_test = apple_full.sample(frac=0.5, random_state=42).reset_index(drop=True)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPRE0R5GvGsr"
      },
      "source": [
        "def get_counts(test):\n",
        "  # print(apple_test)\n",
        "  predictions = saved_model.predict(get_features(test[:5000].text))\n",
        "  # sig_preds = tf.sigmoid(predictions)\n",
        "  sig_preds = np.where(predictions>=0.5, 1, 0)\n",
        "\n",
        "  print(classification_report(test[:5000].label, sig_preds))\n",
        "  print(confusion_matrix(test[:5000].label, sig_preds))\n",
        "\n",
        "  a_df = pd.DataFrame({\"sites\":test[:5000][sig_preds.flatten() == test[:5000].label].source})\n",
        "  print(a_df.sites.value_counts())\n",
        "  print(test[:5000].source.value_counts())"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bYXc4olvMHI",
        "outputId": "c2d0f2bd-8fda-44f8-d677-652aeac0b630"
      },
      "source": [
        "get_counts(apple_test)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [00:02<00:00, 2375.87it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.30      0.39      3007\n",
            "           1       0.38      0.64      0.47      1993\n",
            "\n",
            "    accuracy                           0.44      5000\n",
            "   macro avg       0.47      0.47      0.43      5000\n",
            "weighted avg       0.49      0.44      0.42      5000\n",
            "\n",
            "[[ 903 2104]\n",
            " [ 719 1274]]\n",
            "www.yahoo.com             1003\n",
            "seekingalpha.com           620\n",
            "www.businesswire.com       243\n",
            "investingnews.com          169\n",
            "news.morningstar.com       124\n",
            "www.thestreet.com           12\n",
            "www.investors.com            3\n",
            "seoland.in                   1\n",
            "freeamericanetwork.com       1\n",
            "www.fool.com                 1\n",
            "Name: sites, dtype: int64\n",
            "www.yahoo.com             3124\n",
            "seekingalpha.com          1033\n",
            "www.businesswire.com       332\n",
            "investingnews.com          256\n",
            "news.morningstar.com       225\n",
            "www.thestreet.com           20\n",
            "freeamericanetwork.com       4\n",
            "www.investors.com            4\n",
            "seoland.in                   1\n",
            "www.fool.com                 1\n",
            "Name: source, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4ujrLaKvROT",
        "outputId": "b7b8a4e7-ae75-4875-d3e5-0999a3b13df7"
      },
      "source": [
        "get_counts(amazon_test)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [00:03<00:00, 1485.54it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.35      0.47      3668\n",
            "           1       0.26      0.64      0.37      1332\n",
            "\n",
            "    accuracy                           0.42      5000\n",
            "   macro avg       0.49      0.49      0.42      5000\n",
            "weighted avg       0.60      0.42      0.44      5000\n",
            "\n",
            "[[1273 2395]\n",
            " [ 483  849]]\n",
            "www.yahoo.com               1096\n",
            "www.investors.com            452\n",
            "seekingalpha.com             390\n",
            "www.thestreet.com             98\n",
            "googlejuices.com              25\n",
            "finance.yahoo.com              9\n",
            "news.morningstar.com           8\n",
            "www.topstocksforum.com         7\n",
            "www.nasdaq.com                 6\n",
            "www.financialnewsusa.com       6\n",
            "chinese.wsj.com                5\n",
            "www.fool.com                   3\n",
            "thefly.com                     3\n",
            "etfdailynews.com               3\n",
            "virtualmining.com              2\n",
            "www.msn.com                    2\n",
            "www.wgmd.com                   1\n",
            "redliontrader.com              1\n",
            "www.foxbusiness.com            1\n",
            "es-us.finanzas.yahoo.com       1\n",
            "www.reuters.com                1\n",
            "www.sunmag.net                 1\n",
            "www.businessinsider.com        1\n",
            "Name: sites, dtype: int64\n",
            "www.yahoo.com                    3226\n",
            "www.investors.com                 895\n",
            "seekingalpha.com                  567\n",
            "www.thestreet.com                 140\n",
            "googlejuices.com                   40\n",
            "news.morningstar.com               21\n",
            "www.financialnewsusa.com           15\n",
            "finance.yahoo.com                  14\n",
            "www.topstocksforum.com             10\n",
            "chinese.wsj.com                    10\n",
            "www.nasdaq.com                      9\n",
            "thefly.com                          9\n",
            "in.reuters.com                      6\n",
            "etfdailynews.com                    5\n",
            "www.wgmd.com                        4\n",
            "virtualmining.com                   4\n",
            "www.msn.com                         4\n",
            "www.fool.com                        3\n",
            "uk.reuters.com                      3\n",
            "www.sunmag.net                      3\n",
            "www.foxbusiness.com                 2\n",
            "es-us.finanzas.yahoo.com            2\n",
            "www.reuters.com                     2\n",
            "www.ooyuz.com                       1\n",
            "www.zacks.com                       1\n",
            "redliontrader.com                   1\n",
            "www.benzinga.com                    1\n",
            "www.military-technologies.net       1\n",
            "www.businessinsider.com             1\n",
            "Name: source, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVM8vD0cHGiW",
        "outputId": "7a404f1f-6ff0-46f8-cab3-fdba4250b67f"
      },
      "source": [
        "import json\n",
        "\n",
        "amazon_full = pd.read_csv(\"/content/drive/MyDrive/Amazon_csvs_published_source/Label46623.csv\")\n",
        "apple_full = pd.read_csv(\"/content/drive/MyDrive/apple_csvs_published_source/Label30000.csv\")\n",
        "\n",
        "amazon_full = amazon_full[amazon_full.text.notnull()]\n",
        "apple_full = apple_full[apple_full.text.notnull()]\n",
        "\n",
        "amazon_test = amazon_full.sample(frac=0.5, random_state=42).reset_index(drop=True)\n",
        "apple_test = apple_full.sample(frac=0.5, random_state=42).reset_index(drop=True)\n",
        "\n",
        "test_data = pd.concat([apple_test, amazon_test])\n",
        "test_data = test_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "### Run your testing logic here (generate features...)\n",
        "predictions = saved_model.predict(get_features(test_data.text))\n",
        "# sig_preds = tf.sigmoid(predictions)\n",
        "sig_preds = np.where(predictions>=0.5, 1, 0)\n",
        "\n",
        "print(classification_report(test_data.label, sig_preds))\n",
        "print(confusion_matrix(test_data.label, sig_preds))\n",
        "\n",
        "results_df = pd.DataFrame({\"site\":test_data.source, \"ground_truth\":test_data.label, \"predicted\":sig_preds.flatten()})\n",
        "### Check until here\n",
        "\n",
        "results_df[\"correct\"] = np.where(results_df.ground_truth == results_df.predicted, 1, 0)\n",
        "results_df = results_df.drop(columns=['ground_truth', 'predicted'])\n",
        "\n",
        "site_stats = results_df.groupby([\"site\"]).agg(['count', 'sum'])\n",
        "site_stats.columns = ['_'.join(col) for col in site_stats.columns.values]\n",
        "site_stats.reset_index(inplace=True)\n",
        "site_stats.rename(columns={\"correct_count\":\"support\",\"correct_sum\":\"correct\"}, inplace=True)\n",
        "site_stats[\"accuracy\"] = 100 * site_stats.correct/site_stats.support\n",
        "\n",
        "site_stats_50 = site_stats[site_stats.support > 50]\n",
        "\n",
        "top_10_sites = site_stats_50.sort_values(by=['accuracy'], ascending=False).iloc[:10]\n",
        "\n",
        "for index, row in top_10_sites.iterrows():\n",
        "  d = {\"site\":str(row.site), \"accuracy\":str(row.accuracy), \"support\":str(row.support)}\n",
        "  print(json.dumps(d)+\",\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 38312/38312 [00:23<00:00, 1631.41it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.33      0.44     25859\n",
            "           1       0.31      0.63      0.42     12453\n",
            "\n",
            "    accuracy                           0.43     38312\n",
            "   macro avg       0.48      0.48      0.43     38312\n",
            "weighted avg       0.54      0.43      0.43     38312\n",
            "\n",
            "[[ 8576 17283]\n",
            " [ 4571  7882]]\n",
            "{\"site\": \"www.businesswire.com\", \"accuracy\": \"75.7936507936508\", \"support\": \"1008\"},\n",
            "{\"site\": \"www.thestreet.com\", \"accuracy\": \"67.99007444168734\", \"support\": \"806\"},\n",
            "{\"site\": \"investingnews.com\", \"accuracy\": \"63.41756919374248\", \"support\": \"831\"},\n",
            "{\"site\": \"seekingalpha.com\", \"accuracy\": \"62.60578279266573\", \"support\": \"5672\"},\n",
            "{\"site\": \"www.topstocksforum.com\", \"accuracy\": \"58.62068965517241\", \"support\": \"58\"},\n",
            "{\"site\": \"news.morningstar.com\", \"accuracy\": \"56.809815950920246\", \"support\": \"815\"},\n",
            "{\"site\": \"finance.yahoo.com\", \"accuracy\": \"53.96825396825397\", \"support\": \"63\"},\n",
            "{\"site\": \"googlejuices.com\", \"accuracy\": \"53.55450236966825\", \"support\": \"211\"},\n",
            "{\"site\": \"www.investors.com\", \"accuracy\": \"50.74592074592075\", \"support\": \"4290\"},\n",
            "{\"site\": \"www.financialnewsusa.com\", \"accuracy\": \"50.0\", \"support\": \"52\"},\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gL4JJc6G6TEW"
      },
      "source": [
        "from colabcode import ColabCode\n",
        "server = ColabCode(port=10000, code=False)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-l1qYBq6T3g",
        "outputId": "d78a4e5b-3d21-4acf-c130-6c10f143f9ce"
      },
      "source": [
        "server.run_app(app)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:     Started server process [359]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://127.0.0.1:10000 (Press CTRL+C to quit)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Public URL: NgrokTunnel: \"http://669ef4cd6374.ngrok.io\" -> \"http://localhost:10000\"\n",
            "INFO:     72.223.1.23:0 - \"GET / HTTP/1.1\" 200 OK\n",
            "INFO:     72.223.1.23:0 - \"GET / HTTP/1.1\" 200 OK\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 544.57it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 540.29it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:     72.223.1.23:0 - \"POST /predict HTTP/1.1\" 200 OK\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 1/1 [00:00<00:00, 169.57it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:     72.223.1.23:0 - \"POST /predict HTTP/1.1\" 200 OK\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 1/1 [00:00<00:00, 388.25it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:     72.223.1.23:0 - \"POST /predict HTTP/1.1\" 200 OK\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 1/1 [00:00<00:00, 182.65it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:     72.223.1.23:0 - \"POST /predict HTTP/1.1\" 200 OK\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 1/1 [00:00<00:00, 190.44it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:     72.223.1.23:0 - \"POST /predict HTTP/1.1\" 200 OK\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [359]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7Gv7IyT9cPA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}