{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NB_API.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arpith01/directional_stock_prediction/blob/master/notebooks/models/NB_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GLjPuAR4fji"
      },
      "source": [
        "!pip install colabcode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcvF3aVK4jg0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d2c5b2f-7157-406d-b272-c4fd7e569651"
      },
      "source": [
        "!pip install fastapi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fastapi\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/33/1b643f650688ad368983bbaf3b0658438038ea84d775dd37393d826c3833/fastapi-0.63.0-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 3.5MB/s \n",
            "\u001b[?25hCollecting starlette==0.13.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c5/a4/c9e228d7d47044ce4c83ba002f28ff479e542455f0499198a3f77c94f564/starlette-0.13.6-py3-none-any.whl (59kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 5.0MB/s \n",
            "\u001b[?25hCollecting pydantic<2.0.0,>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/a3/0ffdb6c63f45f10d19b8e8b32670b22ed089cafb29732f6bf8ce518821fb/pydantic-1.8.1-cp37-cp37m-manylinux2014_x86_64.whl (10.1MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1MB 7.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from pydantic<2.0.0,>=1.0.0->fastapi) (3.7.4.3)\n",
            "Installing collected packages: starlette, pydantic, fastapi\n",
            "Successfully installed fastapi-0.63.0 pydantic-1.8.1 starlette-0.13.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RPU3_n24pK3",
        "outputId": "91c134cd-481f-4d22-fd9d-948a056da485"
      },
      "source": [
        "import unicodedata\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.downloader.download('vader_lexicon')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import LancasterStemmer,PorterStemmer\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from tqdm import tqdm\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "from nltk.stem import LancasterStemmer,PorterStemmer\n",
        "\n",
        "\n",
        "def nonAsciiChar(words):\n",
        "    words_list=[]\n",
        "    for w in words:\n",
        "        w=re.sub('[^a-zA-Z]+','',re.sub(r'[\\W\\d]','',w.lower()))\n",
        "        format_words=unicodedata.normalize('NFKD', w).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "        words_list.append(format_words)\n",
        "        \n",
        "    return words_list\n",
        "def stemWordsRemoval(words):\n",
        "    stemmer = PorterStemmer()\n",
        "    words_list=[]\n",
        "    for word in words:\n",
        "        word=stemmer.stem(word)\n",
        "        if word not in words_list:\n",
        "            words_list.append(word)\n",
        "    return words_list\n",
        "\n",
        "def stopWordsRemoval(words):\n",
        "    words_list=[]\n",
        "    for w in words:\n",
        "        if w not in stopwords:\n",
        "            words_list.append(w)\n",
        "    return words_list\n",
        "\n",
        "def removeLinks(words):\n",
        "    words_list=[]\n",
        "    for w in words:\n",
        "        if not re.match('[www]',w):\n",
        "            words_list.append(w)\n",
        "    return words_list\n",
        "\n",
        "def spaceRemoval(words):\n",
        "    words_list=[]\n",
        "    for w in words:\n",
        "        if w!='':\n",
        "            words_list.append(w)\n",
        "    return words_list\n",
        "\n",
        "\n",
        "def dataExtraction(words):\n",
        "    words=nonAsciiChar(words)\n",
        "    words=spaceRemoval(words)\n",
        "    words=stopWordsRemoval(words)\n",
        "    words=stemWordsRemoval(words)\n",
        "    words=removeLinks(words)\n",
        "    return words"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opwDmjVQ4yZc"
      },
      "source": [
        "def get_sentences(news_article):\n",
        "    paragraphs = nltk.sent_tokenize(news_article.lower())\n",
        "    sentences=[]\n",
        "    is_amazon = []\n",
        "    amzn_sentences = []\n",
        "    apple_sentences = []\n",
        "    key_words=['amazon','apple','aapl','amzn']\n",
        "    amazon_keywords = ['amazon', 'amzn']\n",
        "    apple_keywords = ['apple', 'aapl']\n",
        "\n",
        "    for para in paragraphs:\n",
        "      sentences_list = para.split(\"\\n\")\n",
        "      for sentence in sentences_list:\n",
        "        if any(key in sentence for key in amazon_keywords):\n",
        "          sentences.append(sentence)\n",
        "          is_amazon.append(1)\n",
        "        if any(key in sentence for key in apple_keywords):\n",
        "          sentences.append(sentence)\n",
        "          is_amazon.append(0)\n",
        "          # if any(key in sentence for key in key_words):\n",
        "          #   sentences.append(sentence)\n",
        "\n",
        "\n",
        "    if len(sentences) == 0:\n",
        "      return None\n",
        "    \n",
        "    sentences_df = pd.DataFrame({'text':sentences, 'is_amazon':is_amazon})\n",
        "\n",
        "    sentences_df=sentences_df[sentences_df['text'].str.match('^[A-Z a-z 0-9]+')]\n",
        "\n",
        "    sentences_df=sentences_df.drop_duplicates(keep=False).reset_index(drop=True)\n",
        "\n",
        "    sentences_df['words'] = sentences_df.text.apply(word_tokenize)\n",
        "    sentences_df['words'] = sentences_df.words.apply(dataExtraction)\n",
        "    sentences_df['text'] = sentences_df.words.apply(lambda words: \" \".join(words))\n",
        "    sentences_df = sentences_df.drop(columns=\"words\")\n",
        "\n",
        "    return sentences_df"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfTZUVKe47HO"
      },
      "source": [
        "from pydantic import BaseModel\n",
        "from fastapi import FastAPI"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27ozejKW495A"
      },
      "source": [
        "from fastapi.middleware.cors import CORSMiddleware"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7r_rD8t5Aav"
      },
      "source": [
        "class Sentence(BaseModel):\n",
        "  news_article:str"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7UY81Zp5CY6"
      },
      "source": [
        "class Prediction(BaseModel):\n",
        "  prediction_apple:str\n",
        "  confidence_apple:str\n",
        "  prediction_amazon:str\n",
        "  confidence_amazon:str"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiVwdgpb5EG5"
      },
      "source": [
        "app = FastAPI()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pf5379tT5HgQ"
      },
      "source": [
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWKMuHGf825b",
        "outputId": "44e0cd94-2f6b-4e89-edd9-e7f55e7f98c1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZp5X5HoMZD6"
      },
      "source": [
        "##Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVJ0s3R780iE",
        "outputId": "7978b56f-9009-4d40-8473-114417afe862"
      },
      "source": [
        "import pickle\n",
        "path = \"/content/drive/MyDrive/new_data_ARS/naive_bayes.sav\"\n",
        "infile = open(path,'rb')\n",
        "Naive_Bayes = pickle.load(infile)\n",
        "infile.close()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator GaussianNB from version 0.23.2 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
            "  UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inCiVPcV5RKi",
        "outputId": "1704cdc5-b6e7-4d2b-a48a-ac72254d3195"
      },
      "source": [
        "saved_model = Naive_Bayes\n",
        "saved_model"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GaussianNB(priors=None, var_smoothing=1e-09)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blxNHPhBJZC3"
      },
      "source": [
        "path = \"/content/drive/MyDrive/new_data_ARS/vectorizer\"\n",
        "infile = open(path,'rb')\n",
        "vectorizer_dict = pickle.load(infile)\n",
        "infile.close()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhqy-CpOKjtX"
      },
      "source": [
        "vectorizer = vectorizer_dict['vectorizer']"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3rf5W7gKuUq",
        "outputId": "f55017fc-16d1-4e1d-97d9-3a5460701744"
      },
      "source": [
        "vectorizer"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
              "                lowercase=True, max_df=1.0, max_features=4000, min_df=10,\n",
              "                ngram_range=(2, 2), preprocessor=None, stop_words=None,\n",
              "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                tokenizer=None, vocabulary=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4OnyV2KCEdT"
      },
      "source": [
        "def get_features(X_test):\n",
        "  cls = SentimentIntensityAnalyzer()\n",
        "  Positive_X_test_text = []\n",
        "  Negative_X_test_text = []\n",
        "\n",
        "  for i in tqdm(X_test):\n",
        "      n1= cls.polarity_scores(i)[\"neg\"]\n",
        "      n2= cls.polarity_scores(i)[\"pos\"]\n",
        "      Positive_X_test_text.append(n1)\n",
        "      Negative_X_test_text.append(n2)\n",
        "  \n",
        "  # vectorizer = CountVectorizer(ngram_range = (2,2),max_features = 4000, min_df = 10)\n",
        "  \n",
        "  X_test_2gram_features = vectorizer.transform(X_test)\n",
        "  Sentiment_X_test_text = np.column_stack((Positive_X_test_text, Negative_X_test_text))\n",
        "\n",
        "  X_test_2gram_features_array = X_test_2gram_features.toarray()\n",
        "  Xtest = np.concatenate((Sentiment_X_test_text,X_test_2gram_features_array), axis = 1)\n",
        "  return Xtest"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaaLKe-55Jb9"
      },
      "source": [
        "@app.on_event(\"startup\")\n",
        "def load_model_from_file():\n",
        "  global inference_model\n",
        "  inference_model = saved_model\n",
        "\n",
        "\n",
        "@app.get('/')\n",
        "def index():\n",
        "  return{'message':'Post an article to this API to get inference'}\n",
        "\n",
        "@app.post('/predict')\n",
        "def classify_sentence(data:Sentence):\n",
        "  received_data = data.dict()\n",
        "  news_article = received_data['news_article']\n",
        "  sentences = get_sentences(news_article)\n",
        "  prediction_class_amzn = \"-1\"\n",
        "  prediction_class_apple = \"-1\"\n",
        "  confidence_amzn = '0'\n",
        "  confidence_apple = '0'\n",
        "\n",
        "  if sentences is None:\n",
        "    p = Prediction(prediction_apple=prediction_class_apple, confidence_apple=confidence_apple, prediction_amazon = prediction_class_amzn, confidence_amazon=confidence_amzn)\n",
        "    return p\n",
        "\n",
        "\n",
        "  amazon_sentences = sentences[sentences['is_amazon'] == 1].text\n",
        "  apple_sentences = sentences[sentences['is_amazon'] == 0].text\n",
        "\n",
        "  # posterior_prob = tf.sigmoid(inference_model.predict(sentences))\n",
        "\n",
        "  if(len(amazon_sentences) != 0):\n",
        "    amazon_sentences_test = get_features(amazon_sentences)\n",
        "    posterior_prob_amzn = inference_model.predict(amazon_sentences_test)\n",
        "    prediction_class_amzn = np.where(posterior_prob_amzn>0.5, 1, 0)\n",
        "    bin_count_amzn = np.bincount(prediction_class_amzn.flatten())\n",
        "\n",
        "    prediction_class_amzn = np.bincount(prediction_class_amzn.flatten()).argmax()\n",
        "\n",
        "    confidence_amzn = bin_count_amzn[prediction_class_amzn]/np.sum(bin_count_amzn)\n",
        "    confidence_amzn = str(confidence_amzn)\n",
        "    prediction_class_amzn = str(prediction_class_amzn)\n",
        "\n",
        "\n",
        "  if(len(apple_sentences) != 0):\n",
        "    apple_sentences_test = get_features(apple_sentences)\n",
        "    posterior_prob_apple = inference_model.predict(apple_sentences_test)\n",
        "    prediction_class_apple = np.where(posterior_prob_apple>0.5, 1, 0)\n",
        "    bin_count_apple = np.bincount(prediction_class_apple.flatten())\n",
        "\n",
        "    prediction_class_apple = np.bincount(prediction_class_apple.flatten()).argmax()\n",
        "\n",
        "    confidence_apple = bin_count_apple[prediction_class_apple]/np.sum(bin_count_apple)\n",
        "    prediction_class_apple = str(prediction_class_apple)\n",
        "    confidence_apple = str(confidence_apple)\n",
        "\n",
        "  # prediction_classes = np.where(posterior_prob>0.5, 1, 0)\n",
        "\n",
        "  # print(prediction_classes)\n",
        "  # prediction_class = np.bincount(prediction_classes.flatten()).argmax()\n",
        "  # p = Prediction(prediction=str(prediction_class))\n",
        "  p = Prediction(prediction_apple=prediction_class_apple, confidence_apple=confidence_apple, prediction_amazon = prediction_class_amzn, confidence_amazon=confidence_amzn)\n",
        "\n",
        "  return p"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoeADpmotKSX"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "amazon_full = pd.read_csv(\"/content/drive/MyDrive/Amazon_csvs_published_source/Label46623.csv\")\n",
        "\n",
        "apple_full = pd.read_csv(\"/content/drive/MyDrive/apple_csvs_published_source/Label30000.csv\")\n",
        "\n",
        "amazon_full = amazon_full[amazon_full.text.notnull()]\n",
        "apple_full = apple_full[apple_full.text.notnull()]\n",
        "\n",
        "amazon_test = amazon_full.sample(frac=0.5, random_state=42).reset_index(drop=True)\n",
        "apple_test = apple_full.sample(frac=0.5, random_state=42).reset_index(drop=True)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPRE0R5GvGsr"
      },
      "source": [
        "def get_counts(test):\n",
        "  # print(apple_test)\n",
        "  predictions = saved_model.predict(get_features(test[:5000].text))\n",
        "  # sig_preds = tf.sigmoid(predictions)\n",
        "  sig_preds = np.where(predictions>=0.5, 1, 0)\n",
        "\n",
        "  print(classification_report(test[:5000].label, sig_preds))\n",
        "  print(confusion_matrix(test[:5000].label, sig_preds))\n",
        "\n",
        "  a_df = pd.DataFrame({\"sites\":test[:5000][sig_preds.flatten() == test[:5000].label].source})\n",
        "  print(a_df.sites.value_counts())\n",
        "  print(test[:5000].source.value_counts())"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bYXc4olvMHI",
        "outputId": "c1b96278-6507-4ba2-bb40-957c3a47560c"
      },
      "source": [
        "get_counts(apple_test)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [00:02<00:00, 2414.25it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.30      0.39      3007\n",
            "           1       0.38      0.64      0.47      1993\n",
            "\n",
            "    accuracy                           0.44      5000\n",
            "   macro avg       0.47      0.47      0.43      5000\n",
            "weighted avg       0.49      0.44      0.42      5000\n",
            "\n",
            "[[ 903 2104]\n",
            " [ 719 1274]]\n",
            "www.yahoo.com             1003\n",
            "seekingalpha.com           620\n",
            "www.businesswire.com       243\n",
            "investingnews.com          169\n",
            "news.morningstar.com       124\n",
            "www.thestreet.com           12\n",
            "www.investors.com            3\n",
            "seoland.in                   1\n",
            "www.fool.com                 1\n",
            "freeamericanetwork.com       1\n",
            "Name: sites, dtype: int64\n",
            "www.yahoo.com             3124\n",
            "seekingalpha.com          1033\n",
            "www.businesswire.com       332\n",
            "investingnews.com          256\n",
            "news.morningstar.com       225\n",
            "www.thestreet.com           20\n",
            "www.investors.com            4\n",
            "freeamericanetwork.com       4\n",
            "seoland.in                   1\n",
            "www.fool.com                 1\n",
            "Name: source, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4ujrLaKvROT",
        "outputId": "ea5ab32e-78e5-42f6-8a4d-e106279ade35"
      },
      "source": [
        "get_counts(amazon_test)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [00:03<00:00, 1482.82it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.35      0.47      3668\n",
            "           1       0.26      0.64      0.37      1332\n",
            "\n",
            "    accuracy                           0.42      5000\n",
            "   macro avg       0.49      0.49      0.42      5000\n",
            "weighted avg       0.60      0.42      0.44      5000\n",
            "\n",
            "[[1273 2395]\n",
            " [ 483  849]]\n",
            "www.yahoo.com               1096\n",
            "www.investors.com            452\n",
            "seekingalpha.com             390\n",
            "www.thestreet.com             98\n",
            "googlejuices.com              25\n",
            "finance.yahoo.com              9\n",
            "news.morningstar.com           8\n",
            "www.topstocksforum.com         7\n",
            "www.nasdaq.com                 6\n",
            "www.financialnewsusa.com       6\n",
            "chinese.wsj.com                5\n",
            "etfdailynews.com               3\n",
            "thefly.com                     3\n",
            "www.fool.com                   3\n",
            "virtualmining.com              2\n",
            "www.msn.com                    2\n",
            "www.wgmd.com                   1\n",
            "es-us.finanzas.yahoo.com       1\n",
            "www.foxbusiness.com            1\n",
            "www.businessinsider.com        1\n",
            "www.sunmag.net                 1\n",
            "www.reuters.com                1\n",
            "redliontrader.com              1\n",
            "Name: sites, dtype: int64\n",
            "www.yahoo.com                    3226\n",
            "www.investors.com                 895\n",
            "seekingalpha.com                  567\n",
            "www.thestreet.com                 140\n",
            "googlejuices.com                   40\n",
            "news.morningstar.com               21\n",
            "www.financialnewsusa.com           15\n",
            "finance.yahoo.com                  14\n",
            "chinese.wsj.com                    10\n",
            "www.topstocksforum.com             10\n",
            "www.nasdaq.com                      9\n",
            "thefly.com                          9\n",
            "in.reuters.com                      6\n",
            "etfdailynews.com                    5\n",
            "www.wgmd.com                        4\n",
            "virtualmining.com                   4\n",
            "www.msn.com                         4\n",
            "uk.reuters.com                      3\n",
            "www.sunmag.net                      3\n",
            "www.fool.com                        3\n",
            "www.foxbusiness.com                 2\n",
            "es-us.finanzas.yahoo.com            2\n",
            "www.reuters.com                     2\n",
            "www.benzinga.com                    1\n",
            "www.businessinsider.com             1\n",
            "www.military-technologies.net       1\n",
            "www.ooyuz.com                       1\n",
            "www.zacks.com                       1\n",
            "redliontrader.com                   1\n",
            "Name: source, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gL4JJc6G6TEW"
      },
      "source": [
        "from colabcode import ColabCode\n",
        "server = ColabCode(port=10000, code=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-l1qYBq6T3g",
        "outputId": "d78a4e5b-3d21-4acf-c130-6c10f143f9ce"
      },
      "source": [
        "server.run_app(app)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:     Started server process [359]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://127.0.0.1:10000 (Press CTRL+C to quit)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Public URL: NgrokTunnel: \"http://669ef4cd6374.ngrok.io\" -> \"http://localhost:10000\"\n",
            "INFO:     72.223.1.23:0 - \"GET / HTTP/1.1\" 200 OK\n",
            "INFO:     72.223.1.23:0 - \"GET / HTTP/1.1\" 200 OK\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 544.57it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 540.29it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:     72.223.1.23:0 - \"POST /predict HTTP/1.1\" 200 OK\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 1/1 [00:00<00:00, 169.57it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:     72.223.1.23:0 - \"POST /predict HTTP/1.1\" 200 OK\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 1/1 [00:00<00:00, 388.25it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:     72.223.1.23:0 - \"POST /predict HTTP/1.1\" 200 OK\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 1/1 [00:00<00:00, 182.65it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:     72.223.1.23:0 - \"POST /predict HTTP/1.1\" 200 OK\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 1/1 [00:00<00:00, 190.44it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:     72.223.1.23:0 - \"POST /predict HTTP/1.1\" 200 OK\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [359]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7Gv7IyT9cPA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}