{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LR_API.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arpith01/directional_stock_prediction/blob/master/notebooks/models/LR_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GLjPuAR4fji",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ea96a561-f8ea-41fa-a1d5-81708008c6fd"
      },
      "source": [
        "!pip install colabcode"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting colabcode\n",
            "  Downloading https://files.pythonhosted.org/packages/33/f4/5c69125dd58eb86b1c668bf2a449120750b984d03bbcfd583ffcf3bfef48/colabcode-0.2.0-py3-none-any.whl\n",
            "Collecting nest-asyncio==1.4.3\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/33/10805a3359f56ac4f3b520e64b9d5e6a288d87be95777b8023c64cba60f1/nest_asyncio-1.4.3-py3-none-any.whl\n",
            "Collecting pyngrok>=5.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6b/4e/a2fe095bbe17cf26424c4abcd22a0490e22d01cc628f25af5e220ddbf6f0/pyngrok-5.0.5.tar.gz (745kB)\n",
            "\u001b[K     |████████████████████████████████| 747kB 7.7MB/s \n",
            "\u001b[?25hCollecting uvicorn==0.13.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ef/67/546c35e9fffb585ea0608ba3bdcafe17ae402e304367203d0b08d6c23051/uvicorn-0.13.1-py3-none-any.whl (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.4MB/s \n",
            "\u001b[?25hCollecting jupyterlab==3.0.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/27/149c258b8e80552ba1ad35636eca308776a284cb151cb8fcfff70adfbd0a/jupyterlab-3.0.7-py3-none-any.whl (8.3MB)\n",
            "\u001b[K     |████████████████████████████████| 8.3MB 9.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyngrok>=5.0.0->colabcode) (3.13)\n",
            "Collecting h11>=0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/0f/7a0eeea938eaf61074f29fed9717f2010e8d0e0905d36b38d3275a1e4622/h11-0.12.0-py3-none-any.whl (54kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: click==7.* in /usr/local/lib/python3.7/dist-packages (from uvicorn==0.13.1->colabcode) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from uvicorn==0.13.1->colabcode) (3.7.4.3)\n",
            "Collecting jupyterlab-server~=2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/06/3078fdcf2da0a1aa109ccafaab9788c5a35db5be3c042b3eac2cb75d315d/jupyterlab_server-2.4.0-py3-none-any.whl (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: jinja2>=2.10 in /usr/local/lib/python3.7/dist-packages (from jupyterlab==3.0.7->colabcode) (2.11.3)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from jupyterlab==3.0.7->colabcode) (5.5.0)\n",
            "Collecting nbclassic~=0.2\n",
            "  Downloading https://files.pythonhosted.org/packages/e6/f0/113b9a6975e53285c56091a250be4828dd20d80172620a4ff74ed26c9fe7/nbclassic-0.2.6-py3-none-any.whl\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from jupyterlab==3.0.7->colabcode) (20.9)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from jupyterlab==3.0.7->colabcode) (4.7.1)\n",
            "Collecting jupyter-server~=1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/09/60/668563d081a26e1b300e489090f792b394cb8543466d000eb8f6812048da/jupyter_server-1.5.1-py3-none-any.whl (374kB)\n",
            "\u001b[K     |████████████████████████████████| 378kB 43.3MB/s \n",
            "\u001b[?25hCollecting tornado>=6.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/a8/9c5902233fa3c2e6a889cbd164333ddda5009669f494e3fadbeee2c03af5/tornado-6.1-cp37-cp37m-manylinux2010_x86_64.whl (428kB)\n",
            "\u001b[K     |████████████████████████████████| 430kB 37.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: babel in /usr/local/lib/python3.7/dist-packages (from jupyterlab-server~=2.0->jupyterlab==3.0.7->colabcode) (2.9.0)\n",
            "Collecting jsonschema>=3.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c5/8f/51e89ce52a085483359217bc72cdbf6e75ee595d5b1d4b5ade40c7e018b8/jsonschema-3.2.0-py2.py3-none-any.whl (56kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.4MB/s \n",
            "\u001b[?25hCollecting json5\n",
            "  Downloading https://files.pythonhosted.org/packages/2b/81/22bf51a5bc60dde18bb6164fd597f18ee683de8670e141364d9c432dd3cf/json5-0.9.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from jupyterlab-server~=2.0->jupyterlab==3.0.7->colabcode) (2.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2>=2.10->jupyterlab==3.0.7->colabcode) (1.1.1)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->jupyterlab==3.0.7->colabcode) (0.8.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->jupyterlab==3.0.7->colabcode) (54.2.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->jupyterlab==3.0.7->colabcode) (4.4.2)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->jupyterlab==3.0.7->colabcode) (1.0.18)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->jupyterlab==3.0.7->colabcode) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->jupyterlab==3.0.7->colabcode) (5.0.5)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython->jupyterlab==3.0.7->colabcode) (4.8.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->jupyterlab==3.0.7->colabcode) (2.6.1)\n",
            "Requirement already satisfied: notebook<7 in /usr/local/lib/python3.7/dist-packages (from nbclassic~=0.2->jupyterlab==3.0.7->colabcode) (5.3.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->jupyterlab==3.0.7->colabcode) (2.4.7)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (0.9.0)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (20.1.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (5.1.2)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (0.2.0)\n",
            "Collecting jupyter-client>=6.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/d0/9a6cef4d6471e482fbf8943242ddbe559d1f99761e85983c07d623f7b70c/jupyter_client-6.1.13-py3-none-any.whl (112kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 45.4MB/s \n",
            "\u001b[?25hCollecting anyio>=2.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/49/c3/b83a3c02c7d6f66932e9a72621d7f207cbfd2bd72b4c8931567ee386fb55/anyio-2.2.0-py3-none-any.whl (65kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 5.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (22.0.3)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (1.5.0)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (0.9.3)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (5.6.1)\n",
            "Requirement already satisfied: pytz>=2015.7 in /usr/local/lib/python3.7/dist-packages (from babel->jupyterlab-server~=2.0->jupyterlab==3.0.7->colabcode) (2018.9)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0.1->jupyterlab-server~=2.0->jupyterlab==3.0.7->colabcode) (1.15.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0.1->jupyterlab-server~=2.0->jupyterlab==3.0.7->colabcode) (20.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0.1->jupyterlab-server~=2.0->jupyterlab==3.0.7->colabcode) (3.8.1)\n",
            "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0.1->jupyterlab-server~=2.0->jupyterlab==3.0.7->colabcode) (0.17.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->jupyterlab-server~=2.0->jupyterlab==3.0.7->colabcode) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->jupyterlab-server~=2.0->jupyterlab==3.0.7->colabcode) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->jupyterlab-server~=2.0->jupyterlab==3.0.7->colabcode) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->jupyterlab-server~=2.0->jupyterlab==3.0.7->colabcode) (2.10)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->jupyterlab==3.0.7->colabcode) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->jupyterlab==3.0.7->colabcode) (0.7.0)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from notebook<7->nbclassic~=0.2->jupyterlab==3.0.7->colabcode) (4.10.1)\n",
            "Requirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from argon2-cffi->jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (1.14.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=6.1.1->jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (2.8.1)\n",
            "Collecting sniffio>=1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/52/b0/7b2e028b63d092804b6794595871f936aafa5e9322dcaaad50ebf67445b3/sniffio-1.2.0-py3-none-any.whl\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (1.4.3)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (0.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (3.3.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (0.7.1)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (0.4.4)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (0.8.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->jsonschema>=3.0.1->jupyterlab-server~=2.0->jupyterlab==3.0.7->colabcode) (3.4.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0.0->argon2-cffi->jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (2.20)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (0.5.1)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-5.0.5-cp37-none-any.whl size=19246 sha256=fe841894eb4c173de7322a55274794166d0091d3c940625039dc7ef8b65334ef\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/13/64/5ebbcc22eaf53fdf5766b397c1fb17c83f5775fdccf0ea1b88\n",
            "Successfully built pyngrok\n",
            "\u001b[31mERROR: jupyter-client 6.1.13 has requirement nest-asyncio>=1.5, but you'll have nest-asyncio 1.4.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement tornado~=5.1.0; python_version >= \"3.0\", but you'll have tornado 6.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: nest-asyncio, pyngrok, h11, uvicorn, jsonschema, json5, tornado, jupyter-client, sniffio, anyio, jupyter-server, jupyterlab-server, nbclassic, jupyterlab, colabcode\n",
            "  Found existing installation: nest-asyncio 1.5.1\n",
            "    Uninstalling nest-asyncio-1.5.1:\n",
            "      Successfully uninstalled nest-asyncio-1.5.1\n",
            "  Found existing installation: jsonschema 2.6.0\n",
            "    Uninstalling jsonschema-2.6.0:\n",
            "      Successfully uninstalled jsonschema-2.6.0\n",
            "  Found existing installation: tornado 5.1.1\n",
            "    Uninstalling tornado-5.1.1:\n",
            "      Successfully uninstalled tornado-5.1.1\n",
            "  Found existing installation: jupyter-client 5.3.5\n",
            "    Uninstalling jupyter-client-5.3.5:\n",
            "      Successfully uninstalled jupyter-client-5.3.5\n",
            "Successfully installed anyio-2.2.0 colabcode-0.2.0 h11-0.12.0 json5-0.9.5 jsonschema-3.2.0 jupyter-client-6.1.13 jupyter-server-1.5.1 jupyterlab-3.0.7 jupyterlab-server-2.4.0 nbclassic-0.2.6 nest-asyncio-1.4.3 pyngrok-5.0.5 sniffio-1.2.0 tornado-6.1 uvicorn-0.13.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "jupyter_client",
                  "tornado"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcvF3aVK4jg0",
        "outputId": "67677daa-507e-4ec9-c5b0-38897ba0be49"
      },
      "source": [
        "!pip install fastapi"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.7/dist-packages (0.63.0)\n",
            "Requirement already satisfied: starlette==0.13.6 in /usr/local/lib/python3.7/dist-packages (from fastapi) (0.13.6)\n",
            "Requirement already satisfied: pydantic<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from fastapi) (1.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from pydantic<2.0.0,>=1.0.0->fastapi) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RPU3_n24pK3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d342857-6ae3-469d-e44a-a4edf452c630"
      },
      "source": [
        "import unicodedata\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.downloader.download('vader_lexicon')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import LancasterStemmer,PorterStemmer\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from tqdm import tqdm\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "from nltk.stem import LancasterStemmer,PorterStemmer\n",
        "\n",
        "\n",
        "def nonAsciiChar(words):\n",
        "    words_list=[]\n",
        "    for w in words:\n",
        "        w=re.sub('[^a-zA-Z]+','',re.sub(r'[\\W\\d]','',w.lower()))\n",
        "        format_words=unicodedata.normalize('NFKD', w).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "        words_list.append(format_words)\n",
        "        \n",
        "    return words_list\n",
        "def stemWordsRemoval(words):\n",
        "    stemmer = PorterStemmer()\n",
        "    words_list=[]\n",
        "    for word in words:\n",
        "        word=stemmer.stem(word)\n",
        "        if word not in words_list:\n",
        "            words_list.append(word)\n",
        "    return words_list\n",
        "\n",
        "def stopWordsRemoval(words):\n",
        "    words_list=[]\n",
        "    for w in words:\n",
        "        if w not in stopwords:\n",
        "            words_list.append(w)\n",
        "    return words_list\n",
        "\n",
        "def removeLinks(words):\n",
        "    words_list=[]\n",
        "    for w in words:\n",
        "        if not re.match('[www]',w):\n",
        "            words_list.append(w)\n",
        "    return words_list\n",
        "\n",
        "def spaceRemoval(words):\n",
        "    words_list=[]\n",
        "    for w in words:\n",
        "        if w!='':\n",
        "            words_list.append(w)\n",
        "    return words_list\n",
        "\n",
        "\n",
        "def dataExtraction(words):\n",
        "    words=nonAsciiChar(words)\n",
        "    words=spaceRemoval(words)\n",
        "    words=stopWordsRemoval(words)\n",
        "    words=stemWordsRemoval(words)\n",
        "    words=removeLinks(words)\n",
        "    return words"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opwDmjVQ4yZc"
      },
      "source": [
        "def get_sentences(news_article):\n",
        "    paragraphs = nltk.sent_tokenize(news_article.lower())\n",
        "    sentences=[]\n",
        "    is_amazon = []\n",
        "    amzn_sentences = []\n",
        "    apple_sentences = []\n",
        "    key_words=['amazon','apple','aapl','amzn']\n",
        "    amazon_keywords = ['amazon', 'amzn']\n",
        "    apple_keywords = ['apple', 'aapl']\n",
        "\n",
        "    for para in paragraphs:\n",
        "      sentences_list = para.split(\"\\n\")\n",
        "      for sentence in sentences_list:\n",
        "        if any(key in sentence for key in amazon_keywords):\n",
        "          sentences.append(sentence)\n",
        "          is_amazon.append(1)\n",
        "        if any(key in sentence for key in apple_keywords):\n",
        "          sentences.append(sentence)\n",
        "          is_amazon.append(0)\n",
        "          # if any(key in sentence for key in key_words):\n",
        "          #   sentences.append(sentence)\n",
        "\n",
        "\n",
        "    if len(sentences) == 0:\n",
        "      return None\n",
        "    \n",
        "    sentences_df = pd.DataFrame({'text':sentences, 'is_amazon':is_amazon})\n",
        "\n",
        "    sentences_df=sentences_df[sentences_df['text'].str.match('^[A-Z a-z 0-9]+')]\n",
        "\n",
        "    sentences_df=sentences_df.drop_duplicates(keep=False).reset_index(drop=True)\n",
        "\n",
        "    sentences_df['words'] = sentences_df.text.apply(word_tokenize)\n",
        "    sentences_df['words'] = sentences_df.words.apply(dataExtraction)\n",
        "    sentences_df['text'] = sentences_df.words.apply(lambda words: \" \".join(words))\n",
        "    sentences_df = sentences_df.drop(columns=\"words\")\n",
        "\n",
        "    return sentences_df"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfTZUVKe47HO"
      },
      "source": [
        "from pydantic import BaseModel\n",
        "from fastapi import FastAPI"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27ozejKW495A"
      },
      "source": [
        "from fastapi.middleware.cors import CORSMiddleware"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7r_rD8t5Aav"
      },
      "source": [
        "class Sentence(BaseModel):\n",
        "  news_article:str"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7UY81Zp5CY6"
      },
      "source": [
        "class Prediction(BaseModel):\n",
        "  prediction_apple:str\n",
        "  confidence_apple:str\n",
        "  prediction_amazon:str\n",
        "  confidence_amazon:str"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiVwdgpb5EG5"
      },
      "source": [
        "app = FastAPI()"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pf5379tT5HgQ"
      },
      "source": [
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWKMuHGf825b",
        "outputId": "597b92df-1890-47d5-d8ce-faf80f66c8df"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZp5X5HoMZD6"
      },
      "source": [
        "##Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVJ0s3R780iE",
        "outputId": "4d409b51-ff2d-47bf-e5bd-ad1e4fcc4f26"
      },
      "source": [
        "import pickle\n",
        "path = \"/content/drive/MyDrive/new_data_ARS/log_reg.sav\"\n",
        "infile = open(path,'rb')\n",
        "Logistic_Regression = pickle.load(infile)\n",
        "infile.close()"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.23.2 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
            "  UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inCiVPcV5RKi",
        "outputId": "155a6133-b1bf-4dd1-df41-1cff1bf6ae2f"
      },
      "source": [
        "saved_model = Logistic_Regression\n",
        "saved_model"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=0, solver='liblinear', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blxNHPhBJZC3"
      },
      "source": [
        "path = \"/content/drive/MyDrive/new_data_ARS/vectorizer\"\n",
        "infile = open(path,'rb')\n",
        "vectorizer_dict = pickle.load(infile)\n",
        "infile.close()"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhqy-CpOKjtX"
      },
      "source": [
        "vectorizer = vectorizer_dict['vectorizer']"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3rf5W7gKuUq",
        "outputId": "065fea36-eee3-414f-f938-7ab85f09cd36"
      },
      "source": [
        "vectorizer"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
              "                lowercase=True, max_df=1.0, max_features=4000, min_df=10,\n",
              "                ngram_range=(2, 2), preprocessor=None, stop_words=None,\n",
              "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                tokenizer=None, vocabulary=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4OnyV2KCEdT"
      },
      "source": [
        "def get_features(X_test):\n",
        "  cls = SentimentIntensityAnalyzer()\n",
        "  Positive_X_test_text = []\n",
        "  Negative_X_test_text = []\n",
        "\n",
        "  for i in tqdm(X_test):\n",
        "      n1= cls.polarity_scores(i)[\"neg\"]\n",
        "      n2= cls.polarity_scores(i)[\"pos\"]\n",
        "      Positive_X_test_text.append(n1)\n",
        "      Negative_X_test_text.append(n2)\n",
        "  \n",
        "  # vectorizer = CountVectorizer(ngram_range = (2,2),max_features = 4000, min_df = 10)\n",
        "  \n",
        "  X_test_2gram_features = vectorizer.transform(X_test)\n",
        "  Sentiment_X_test_text = np.column_stack((Positive_X_test_text, Negative_X_test_text))\n",
        "\n",
        "  X_test_2gram_features_array = X_test_2gram_features.toarray()\n",
        "  Xtest = np.concatenate((Sentiment_X_test_text,X_test_2gram_features_array), axis = 1)\n",
        "  return Xtest"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaaLKe-55Jb9"
      },
      "source": [
        "@app.on_event(\"startup\")\n",
        "def load_model_from_file():\n",
        "  global inference_model\n",
        "  inference_model = saved_model_path\n",
        "\n",
        "\n",
        "@app.get('/')\n",
        "def index():\n",
        "  return{'message':'Post an article to this API to get inference'}\n",
        "\n",
        "@app.post('/predict')\n",
        "def classify_sentence(data:Sentence):\n",
        "  received_data = data.dict()\n",
        "  news_article = received_data['news_article']\n",
        "  sentences = get_sentences(news_article)\n",
        "  prediction_class_amzn = \"-1\"\n",
        "  prediction_class_apple = \"-1\"\n",
        "  confidence_amzn = '0'\n",
        "  confidence_apple = '0'\n",
        "\n",
        "  if sentences is None:\n",
        "    p = Prediction(prediction_apple=prediction_class_apple, confidence_apple=confidence_apple, prediction_amazon = prediction_class_amzn, confidence_amazon=confidence_amzn)\n",
        "    return p\n",
        "\n",
        "\n",
        "  amazon_sentences = sentences[sentences['is_amazon'] == 1].text\n",
        "  apple_sentences = sentences[sentences['is_amazon'] == 0].text\n",
        "\n",
        "  # posterior_prob = tf.sigmoid(inference_model.predict(sentences))\n",
        "\n",
        "  if(len(amazon_sentences) != 0):\n",
        "    amazon_sentences_test = get_features(amazon_sentences)\n",
        "    posterior_prob_amzn = inference_model.predict(amazon_sentences_test)\n",
        "    prediction_class_amzn = np.where(posterior_prob_amzn>0.5, 1, 0)\n",
        "    bin_count_amzn = np.bincount(prediction_class_amzn.flatten())\n",
        "\n",
        "    prediction_class_amzn = np.bincount(prediction_class_amzn.flatten()).argmax()\n",
        "\n",
        "    confidence_amzn = bin_count_amzn[prediction_class_amzn]/np.sum(bin_count_amzn)\n",
        "    confidence_amzn = str(confidence_amzn)\n",
        "    prediction_class_amzn = str(prediction_class_amzn)\n",
        "\n",
        "\n",
        "  if(len(apple_sentences) != 0):\n",
        "    apple_sentences_test = get_features(apple_sentences)\n",
        "    posterior_prob_apple = inference_model.predict(apple_sentences_test)\n",
        "    prediction_class_apple = np.where(posterior_prob_apple>0.5, 1, 0)\n",
        "    bin_count_apple = np.bincount(prediction_class_apple.flatten())\n",
        "\n",
        "    prediction_class_apple = np.bincount(prediction_class_apple.flatten()).argmax()\n",
        "\n",
        "    confidence_apple = bin_count_apple[prediction_class_apple]/np.sum(bin_count_apple)\n",
        "    prediction_class_apple = str(prediction_class_apple)\n",
        "    confidence_apple = str(confidence_apple)\n",
        "\n",
        "  # prediction_classes = np.where(posterior_prob>0.5, 1, 0)\n",
        "\n",
        "  # print(prediction_classes)\n",
        "  # prediction_class = np.bincount(prediction_classes.flatten()).argmax()\n",
        "  # p = Prediction(prediction=str(prediction_class))\n",
        "  p = Prediction(prediction_apple=prediction_class_apple, confidence_apple=confidence_apple, prediction_amazon = prediction_class_amzn, confidence_amazon=confidence_amzn)\n",
        "\n",
        "  return p"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M45oFMXrs8J0"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "amazon_full = pd.read_csv(\"/content/drive/MyDrive/Amazon_csvs_published_source/Label46623.csv\")\n",
        "\n",
        "apple_full = pd.read_csv(\"/content/drive/MyDrive/apple_csvs_published_source/Label30000.csv\")\n",
        "\n",
        "amazon_full = amazon_full[amazon_full.text.notnull()]\n",
        "apple_full = apple_full[apple_full.text.notnull()]\n",
        "\n",
        "amazon_test = amazon_full.sample(frac=0.5, random_state=42).reset_index(drop=True)\n",
        "apple_test = apple_full.sample(frac=0.5, random_state=42).reset_index(drop=True)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KamDZQEvxfb"
      },
      "source": [
        "def get_counts(test):\n",
        "  # print(apple_test)\n",
        "  predictions = saved_model.predict(get_features(test[:5000].text))\n",
        "  # sig_preds = tf.sigmoid(predictions)\n",
        "  sig_preds = np.where(predictions>=0.5, 1, 0)\n",
        "\n",
        "  print(classification_report(test[:5000].label, sig_preds))\n",
        "  print(confusion_matrix(test[:5000].label, sig_preds))\n",
        "\n",
        "  a_df = pd.DataFrame({\"sites\":test[:5000][sig_preds.flatten() == test[:5000].label].source})\n",
        "  print(a_df.sites.value_counts())\n",
        "  print(test[:5000].source.value_counts())"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stn-dyYXv14Y",
        "outputId": "6f73e756-ce0e-4bd0-d757-84a3e6ccc11a"
      },
      "source": [
        "get_counts(apple_test)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [00:02<00:00, 2272.29it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.68      0.64      3007\n",
            "           1       0.42      0.35      0.38      1993\n",
            "\n",
            "    accuracy                           0.55      5000\n",
            "   macro avg       0.52      0.51      0.51      5000\n",
            "weighted avg       0.54      0.55      0.54      5000\n",
            "\n",
            "[[2048  959]\n",
            " [1299  694]]\n",
            "www.yahoo.com             2050\n",
            "seekingalpha.com           356\n",
            "news.morningstar.com       168\n",
            "investingnews.com           84\n",
            "www.businesswire.com        72\n",
            "www.thestreet.com            9\n",
            "freeamericanetwork.com       2\n",
            "www.fool.com                 1\n",
            "Name: sites, dtype: int64\n",
            "www.yahoo.com             3124\n",
            "seekingalpha.com          1033\n",
            "www.businesswire.com       332\n",
            "investingnews.com          256\n",
            "news.morningstar.com       225\n",
            "www.thestreet.com           20\n",
            "www.investors.com            4\n",
            "freeamericanetwork.com       4\n",
            "www.fool.com                 1\n",
            "seoland.in                   1\n",
            "Name: source, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmB1txVRv41Z",
        "outputId": "d9e928f1-4604-4812-b199-86309eeb8c42"
      },
      "source": [
        "get_counts(amazon_test)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [00:03<00:00, 1443.03it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.83      0.79      3668\n",
            "           1       0.38      0.29      0.33      1332\n",
            "\n",
            "    accuracy                           0.68      5000\n",
            "   macro avg       0.57      0.56      0.56      5000\n",
            "weighted avg       0.66      0.68      0.67      5000\n",
            "\n",
            "[[3032  636]\n",
            " [ 950  382]]\n",
            "www.yahoo.com               2754\n",
            "www.investors.com            389\n",
            "seekingalpha.com             148\n",
            "www.thestreet.com             35\n",
            "googlejuices.com              19\n",
            "news.morningstar.com           9\n",
            "www.topstocksforum.com         8\n",
            "www.nasdaq.com                 7\n",
            "www.financialnewsusa.com       7\n",
            "in.reuters.com                 6\n",
            "finance.yahoo.com              5\n",
            "chinese.wsj.com                4\n",
            "www.wgmd.com                   4\n",
            "uk.reuters.com                 3\n",
            "etfdailynews.com               3\n",
            "www.msn.com                    2\n",
            "es-us.finanzas.yahoo.com       2\n",
            "virtualmining.com              2\n",
            "thefly.com                     2\n",
            "www.zacks.com                  1\n",
            "www.fool.com                   1\n",
            "www.sunmag.net                 1\n",
            "www.benzinga.com               1\n",
            "www.foxbusiness.com            1\n",
            "Name: sites, dtype: int64\n",
            "www.yahoo.com                    3226\n",
            "www.investors.com                 895\n",
            "seekingalpha.com                  567\n",
            "www.thestreet.com                 140\n",
            "googlejuices.com                   40\n",
            "news.morningstar.com               21\n",
            "www.financialnewsusa.com           15\n",
            "finance.yahoo.com                  14\n",
            "chinese.wsj.com                    10\n",
            "www.topstocksforum.com             10\n",
            "thefly.com                          9\n",
            "www.nasdaq.com                      9\n",
            "in.reuters.com                      6\n",
            "etfdailynews.com                    5\n",
            "www.wgmd.com                        4\n",
            "virtualmining.com                   4\n",
            "www.msn.com                         4\n",
            "www.sunmag.net                      3\n",
            "www.fool.com                        3\n",
            "uk.reuters.com                      3\n",
            "www.reuters.com                     2\n",
            "es-us.finanzas.yahoo.com            2\n",
            "www.foxbusiness.com                 2\n",
            "redliontrader.com                   1\n",
            "www.benzinga.com                    1\n",
            "www.businessinsider.com             1\n",
            "www.ooyuz.com                       1\n",
            "www.military-technologies.net       1\n",
            "www.zacks.com                       1\n",
            "Name: source, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gL4JJc6G6TEW"
      },
      "source": [
        "from colabcode import ColabCode\n",
        "server = ColabCode(port=10000, code=False)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-l1qYBq6T3g",
        "outputId": "431dd249-9631-49bd-f5f3-d4fb54926374"
      },
      "source": [
        "server.run_app(app)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Public URL: NgrokTunnel: \"http://565aa61d8efe.ngrok.io\" -> \"http://localhost:10000\"\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:     Started server process [57]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://127.0.0.1:10000 (Press CTRL+C to quit)\n",
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [57]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7Gv7IyT9cPA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}