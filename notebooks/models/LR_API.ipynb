{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LR_API.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arpith01/directional_stock_prediction/blob/master/notebooks/models/LR_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GLjPuAR4fji"
      },
      "source": [
        "!pip install colabcode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcvF3aVK4jg0",
        "outputId": "5c0d89aa-1e6b-44a2-d554-5b14104988f1"
      },
      "source": [
        "!pip install fastapi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fastapi\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/33/1b643f650688ad368983bbaf3b0658438038ea84d775dd37393d826c3833/fastapi-0.63.0-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 2.4MB/s \n",
            "\u001b[?25hCollecting pydantic<2.0.0,>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/a3/0ffdb6c63f45f10d19b8e8b32670b22ed089cafb29732f6bf8ce518821fb/pydantic-1.8.1-cp37-cp37m-manylinux2014_x86_64.whl (10.1MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1MB 5.7MB/s \n",
            "\u001b[?25hCollecting starlette==0.13.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c5/a4/c9e228d7d47044ce4c83ba002f28ff479e542455f0499198a3f77c94f564/starlette-0.13.6-py3-none-any.whl (59kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 4.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from pydantic<2.0.0,>=1.0.0->fastapi) (3.7.4.3)\n",
            "Installing collected packages: pydantic, starlette, fastapi\n",
            "Successfully installed fastapi-0.63.0 pydantic-1.8.1 starlette-0.13.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RPU3_n24pK3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "117cd935-96d4-4c65-c74c-b67b0f54bb16"
      },
      "source": [
        "import unicodedata\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.downloader.download('vader_lexicon')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import LancasterStemmer,PorterStemmer\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from tqdm import tqdm\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "from nltk.stem import LancasterStemmer,PorterStemmer\n",
        "\n",
        "\n",
        "def nonAsciiChar(words):\n",
        "    words_list=[]\n",
        "    for w in words:\n",
        "        w=re.sub('[^a-zA-Z]+','',re.sub(r'[\\W\\d]','',w.lower()))\n",
        "        format_words=unicodedata.normalize('NFKD', w).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "        words_list.append(format_words)\n",
        "        \n",
        "    return words_list\n",
        "def stemWordsRemoval(words):\n",
        "    stemmer = PorterStemmer()\n",
        "    words_list=[]\n",
        "    for word in words:\n",
        "        word=stemmer.stem(word)\n",
        "        if word not in words_list:\n",
        "            words_list.append(word)\n",
        "    return words_list\n",
        "\n",
        "def stopWordsRemoval(words):\n",
        "    words_list=[]\n",
        "    for w in words:\n",
        "        if w not in stopwords:\n",
        "            words_list.append(w)\n",
        "    return words_list\n",
        "\n",
        "def removeLinks(words):\n",
        "    words_list=[]\n",
        "    for w in words:\n",
        "        if not re.match('[www]',w):\n",
        "            words_list.append(w)\n",
        "    return words_list\n",
        "\n",
        "def spaceRemoval(words):\n",
        "    words_list=[]\n",
        "    for w in words:\n",
        "        if w!='':\n",
        "            words_list.append(w)\n",
        "    return words_list\n",
        "\n",
        "\n",
        "def dataExtraction(words):\n",
        "    words=nonAsciiChar(words)\n",
        "    words=spaceRemoval(words)\n",
        "    words=stopWordsRemoval(words)\n",
        "    words=stemWordsRemoval(words)\n",
        "    words=removeLinks(words)\n",
        "    return words"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opwDmjVQ4yZc"
      },
      "source": [
        "def get_sentences(news_article):\n",
        "    paragraphs = nltk.sent_tokenize(news_article.lower())\n",
        "    sentences=[]\n",
        "    is_amazon = []\n",
        "    amzn_sentences = []\n",
        "    apple_sentences = []\n",
        "    key_words=['amazon','apple','aapl','amzn']\n",
        "    amazon_keywords = ['amazon', 'amzn']\n",
        "    apple_keywords = ['apple', 'aapl']\n",
        "\n",
        "    for para in paragraphs:\n",
        "      sentences_list = para.split(\"\\n\")\n",
        "      for sentence in sentences_list:\n",
        "        if any(key in sentence for key in amazon_keywords):\n",
        "          sentences.append(sentence)\n",
        "          is_amazon.append(1)\n",
        "        if any(key in sentence for key in apple_keywords):\n",
        "          sentences.append(sentence)\n",
        "          is_amazon.append(0)\n",
        "          # if any(key in sentence for key in key_words):\n",
        "          #   sentences.append(sentence)\n",
        "\n",
        "\n",
        "    if len(sentences) == 0:\n",
        "      return None\n",
        "    \n",
        "    sentences_df = pd.DataFrame({'text':sentences, 'is_amazon':is_amazon})\n",
        "\n",
        "    sentences_df=sentences_df[sentences_df['text'].str.match('^[A-Z a-z 0-9]+')]\n",
        "\n",
        "    sentences_df=sentences_df.drop_duplicates(keep=False).reset_index(drop=True)\n",
        "\n",
        "    sentences_df['words'] = sentences_df.text.apply(word_tokenize)\n",
        "    sentences_df['words'] = sentences_df.words.apply(dataExtraction)\n",
        "    sentences_df['text'] = sentences_df.words.apply(lambda words: \" \".join(words))\n",
        "    sentences_df = sentences_df.drop(columns=\"words\")\n",
        "\n",
        "    return sentences_df"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfTZUVKe47HO"
      },
      "source": [
        "from pydantic import BaseModel\n",
        "from fastapi import FastAPI"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27ozejKW495A"
      },
      "source": [
        "from fastapi.middleware.cors import CORSMiddleware"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7r_rD8t5Aav"
      },
      "source": [
        "class Sentence(BaseModel):\n",
        "  news_article:str"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7UY81Zp5CY6"
      },
      "source": [
        "class Prediction(BaseModel):\n",
        "  prediction_apple:str\n",
        "  confidence_apple:str\n",
        "  prediction_amazon:str\n",
        "  confidence_amazon:str"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiVwdgpb5EG5"
      },
      "source": [
        "app = FastAPI()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pf5379tT5HgQ"
      },
      "source": [
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWKMuHGf825b",
        "outputId": "dda80c96-5f39-47c2-fdf7-46fe607264a4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZp5X5HoMZD6"
      },
      "source": [
        "##Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVJ0s3R780iE",
        "outputId": "7e23966d-26b4-444e-be24-7d78fa901343"
      },
      "source": [
        "import pickle\n",
        "path = \"/content/drive/MyDrive/new_data_ARS/log_reg.sav\"\n",
        "infile = open(path,'rb')\n",
        "Logistic_Regression = pickle.load(infile)\n",
        "infile.close()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.23.2 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
            "  UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inCiVPcV5RKi",
        "outputId": "bc14a009-fea8-4ecc-a632-2d43c6861370"
      },
      "source": [
        "saved_model = Logistic_Regression\n",
        "saved_model"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=0, solver='liblinear', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blxNHPhBJZC3"
      },
      "source": [
        "path = \"/content/drive/MyDrive/new_data_ARS/vectorizer\"\n",
        "infile = open(path,'rb')\n",
        "vectorizer_dict = pickle.load(infile)\n",
        "infile.close()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhqy-CpOKjtX"
      },
      "source": [
        "vectorizer = vectorizer_dict['vectorizer']"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3rf5W7gKuUq",
        "outputId": "d32dd5e7-4af8-4cdc-8507-8f57ea259588"
      },
      "source": [
        "vectorizer"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
              "                lowercase=True, max_df=1.0, max_features=4000, min_df=10,\n",
              "                ngram_range=(2, 2), preprocessor=None, stop_words=None,\n",
              "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                tokenizer=None, vocabulary=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4OnyV2KCEdT"
      },
      "source": [
        "def get_features(X_test):\n",
        "  cls = SentimentIntensityAnalyzer()\n",
        "  Positive_X_test_text = []\n",
        "  Negative_X_test_text = []\n",
        "\n",
        "  for i in tqdm(X_test):\n",
        "      n1= cls.polarity_scores(i)[\"neg\"]\n",
        "      n2= cls.polarity_scores(i)[\"pos\"]\n",
        "      Positive_X_test_text.append(n1)\n",
        "      Negative_X_test_text.append(n2)\n",
        "  \n",
        "  # vectorizer = CountVectorizer(ngram_range = (2,2),max_features = 4000, min_df = 10)\n",
        "  \n",
        "  X_test_2gram_features = vectorizer.transform(X_test)\n",
        "  Sentiment_X_test_text = np.column_stack((Positive_X_test_text, Negative_X_test_text))\n",
        "\n",
        "  X_test_2gram_features_array = X_test_2gram_features.toarray()\n",
        "  Xtest = np.concatenate((Sentiment_X_test_text,X_test_2gram_features_array), axis = 1)\n",
        "  return Xtest"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaaLKe-55Jb9"
      },
      "source": [
        "@app.on_event(\"startup\")\n",
        "def load_model_from_file():\n",
        "  global inference_model\n",
        "  inference_model = saved_model_path\n",
        "\n",
        "\n",
        "@app.get('/')\n",
        "def index():\n",
        "  return{'message':'Post an article to this API to get inference'}\n",
        "\n",
        "@app.post('/predict')\n",
        "def classify_sentence(data:Sentence):\n",
        "  received_data = data.dict()\n",
        "  news_article = received_data['news_article']\n",
        "  sentences = get_sentences(news_article)\n",
        "  prediction_class_amzn = \"-1\"\n",
        "  prediction_class_apple = \"-1\"\n",
        "  confidence_amzn = '0'\n",
        "  confidence_apple = '0'\n",
        "\n",
        "  if sentences is None:\n",
        "    p = Prediction(prediction_apple=prediction_class_apple, confidence_apple=confidence_apple, prediction_amazon = prediction_class_amzn, confidence_amazon=confidence_amzn)\n",
        "    return p\n",
        "\n",
        "\n",
        "  amazon_sentences = sentences[sentences['is_amazon'] == 1].text\n",
        "  apple_sentences = sentences[sentences['is_amazon'] == 0].text\n",
        "\n",
        "  # posterior_prob = tf.sigmoid(inference_model.predict(sentences))\n",
        "\n",
        "  if(len(amazon_sentences) != 0):\n",
        "    amazon_sentences_test = get_features(amazon_sentences)\n",
        "    posterior_prob_amzn = inference_model.predict(amazon_sentences_test)\n",
        "    prediction_class_amzn = np.where(posterior_prob_amzn>0.5, 1, 0)\n",
        "    bin_count_amzn = np.bincount(prediction_class_amzn.flatten())\n",
        "\n",
        "    prediction_class_amzn = np.bincount(prediction_class_amzn.flatten()).argmax()\n",
        "\n",
        "    confidence_amzn = bin_count_amzn[prediction_class_amzn]/np.sum(bin_count_amzn)\n",
        "    confidence_amzn = str(confidence_amzn)\n",
        "    prediction_class_amzn = str(prediction_class_amzn)\n",
        "\n",
        "\n",
        "  if(len(apple_sentences) != 0):\n",
        "    apple_sentences_test = get_features(apple_sentences)\n",
        "    posterior_prob_apple = inference_model.predict(apple_sentences_test)\n",
        "    prediction_class_apple = np.where(posterior_prob_apple>0.5, 1, 0)\n",
        "    bin_count_apple = np.bincount(prediction_class_apple.flatten())\n",
        "\n",
        "    prediction_class_apple = np.bincount(prediction_class_apple.flatten()).argmax()\n",
        "\n",
        "    confidence_apple = bin_count_apple[prediction_class_apple]/np.sum(bin_count_apple)\n",
        "    prediction_class_apple = str(prediction_class_apple)\n",
        "    confidence_apple = str(confidence_apple)\n",
        "\n",
        "  # prediction_classes = np.where(posterior_prob>0.5, 1, 0)\n",
        "\n",
        "  # print(prediction_classes)\n",
        "  # prediction_class = np.bincount(prediction_classes.flatten()).argmax()\n",
        "  # p = Prediction(prediction=str(prediction_class))\n",
        "  p = Prediction(prediction_apple=prediction_class_apple, confidence_apple=confidence_apple, prediction_amazon = prediction_class_amzn, confidence_amazon=confidence_amzn)\n",
        "\n",
        "  return p"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M45oFMXrs8J0"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "amazon_full = pd.read_csv(\"/content/drive/MyDrive/Amazon_csvs_published_source/Label46623.csv\")\n",
        "\n",
        "apple_full = pd.read_csv(\"/content/drive/MyDrive/apple_csvs_published_source/Label30000.csv\")\n",
        "\n",
        "amazon_full = amazon_full[amazon_full.text.notnull()]\n",
        "apple_full = apple_full[apple_full.text.notnull()]\n",
        "\n",
        "amazon_test = amazon_full.sample(frac=0.5, random_state=42).reset_index(drop=True)\n",
        "apple_test = apple_full.sample(frac=0.5, random_state=42).reset_index(drop=True)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KamDZQEvxfb"
      },
      "source": [
        "def get_counts(test):\n",
        "  # print(apple_test)\n",
        "  predictions = saved_model.predict(get_features(test[:5000].text))\n",
        "  # sig_preds = tf.sigmoid(predictions)\n",
        "  sig_preds = np.where(predictions>=0.5, 1, 0)\n",
        "\n",
        "  print(classification_report(test[:5000].label, sig_preds))\n",
        "  print(confusion_matrix(test[:5000].label, sig_preds))\n",
        "\n",
        "  a_df = pd.DataFrame({\"sites\":test[:5000][sig_preds.flatten() == test[:5000].label].source})\n",
        "  print(a_df.sites.value_counts())\n",
        "  print(test[:5000].source.value_counts())"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stn-dyYXv14Y",
        "outputId": "9b62a5ba-455d-45c2-9033-f857b6210f74"
      },
      "source": [
        "get_counts(apple_test)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [00:02<00:00, 2331.81it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.68      0.64      3007\n",
            "           1       0.42      0.35      0.38      1993\n",
            "\n",
            "    accuracy                           0.55      5000\n",
            "   macro avg       0.52      0.51      0.51      5000\n",
            "weighted avg       0.54      0.55      0.54      5000\n",
            "\n",
            "[[2048  959]\n",
            " [1299  694]]\n",
            "www.yahoo.com             2050\n",
            "seekingalpha.com           356\n",
            "news.morningstar.com       168\n",
            "investingnews.com           84\n",
            "www.businesswire.com        72\n",
            "www.thestreet.com            9\n",
            "freeamericanetwork.com       2\n",
            "www.fool.com                 1\n",
            "Name: sites, dtype: int64\n",
            "www.yahoo.com             3124\n",
            "seekingalpha.com          1033\n",
            "www.businesswire.com       332\n",
            "investingnews.com          256\n",
            "news.morningstar.com       225\n",
            "www.thestreet.com           20\n",
            "www.investors.com            4\n",
            "freeamericanetwork.com       4\n",
            "seoland.in                   1\n",
            "www.fool.com                 1\n",
            "Name: source, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmB1txVRv41Z",
        "outputId": "9c9a60a1-c70e-40e7-8a2d-966dc2fdde62"
      },
      "source": [
        "get_counts(amazon_test)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [00:03<00:00, 1426.08it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.83      0.79      3668\n",
            "           1       0.38      0.29      0.33      1332\n",
            "\n",
            "    accuracy                           0.68      5000\n",
            "   macro avg       0.57      0.56      0.56      5000\n",
            "weighted avg       0.66      0.68      0.67      5000\n",
            "\n",
            "[[3032  636]\n",
            " [ 950  382]]\n",
            "www.yahoo.com               2754\n",
            "www.investors.com            389\n",
            "seekingalpha.com             148\n",
            "www.thestreet.com             35\n",
            "googlejuices.com              19\n",
            "news.morningstar.com           9\n",
            "www.topstocksforum.com         8\n",
            "www.financialnewsusa.com       7\n",
            "www.nasdaq.com                 7\n",
            "in.reuters.com                 6\n",
            "finance.yahoo.com              5\n",
            "www.wgmd.com                   4\n",
            "chinese.wsj.com                4\n",
            "uk.reuters.com                 3\n",
            "etfdailynews.com               3\n",
            "thefly.com                     2\n",
            "www.msn.com                    2\n",
            "virtualmining.com              2\n",
            "es-us.finanzas.yahoo.com       2\n",
            "www.benzinga.com               1\n",
            "www.sunmag.net                 1\n",
            "www.foxbusiness.com            1\n",
            "www.fool.com                   1\n",
            "www.zacks.com                  1\n",
            "Name: sites, dtype: int64\n",
            "www.yahoo.com                    3226\n",
            "www.investors.com                 895\n",
            "seekingalpha.com                  567\n",
            "www.thestreet.com                 140\n",
            "googlejuices.com                   40\n",
            "news.morningstar.com               21\n",
            "www.financialnewsusa.com           15\n",
            "finance.yahoo.com                  14\n",
            "chinese.wsj.com                    10\n",
            "www.topstocksforum.com             10\n",
            "www.nasdaq.com                      9\n",
            "thefly.com                          9\n",
            "in.reuters.com                      6\n",
            "etfdailynews.com                    5\n",
            "virtualmining.com                   4\n",
            "www.wgmd.com                        4\n",
            "www.msn.com                         4\n",
            "uk.reuters.com                      3\n",
            "www.fool.com                        3\n",
            "www.sunmag.net                      3\n",
            "es-us.finanzas.yahoo.com            2\n",
            "www.reuters.com                     2\n",
            "www.foxbusiness.com                 2\n",
            "www.businessinsider.com             1\n",
            "www.military-technologies.net       1\n",
            "redliontrader.com                   1\n",
            "www.benzinga.com                    1\n",
            "www.ooyuz.com                       1\n",
            "www.zacks.com                       1\n",
            "Name: source, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOIdq2qLF0Nd",
        "outputId": "ee64580c-91d0-4843-cca9-8355310a0209"
      },
      "source": [
        "import json\n",
        "\n",
        "amazon_full = pd.read_csv(\"/content/drive/MyDrive/Amazon_csvs_published_source/Label46623.csv\")\n",
        "apple_full = pd.read_csv(\"/content/drive/MyDrive/apple_csvs_published_source/Label30000.csv\")\n",
        "\n",
        "amazon_full = amazon_full[amazon_full.text.notnull()]\n",
        "apple_full = apple_full[apple_full.text.notnull()]\n",
        "\n",
        "amazon_test = amazon_full.sample(frac=0.5, random_state=42).reset_index(drop=True)\n",
        "apple_test = apple_full.sample(frac=0.5, random_state=42).reset_index(drop=True)\n",
        "\n",
        "test_data = pd.concat([apple_test, amazon_test])\n",
        "test_data = test_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "### Run your testing logic here (generate features...)\n",
        "predictions = saved_model.predict(get_features(test_data.text))\n",
        "# sig_preds = tf.sigmoid(predictions)\n",
        "sig_preds = np.where(predictions>=0.5, 1, 0)\n",
        "\n",
        "print(classification_report(test_data.label, sig_preds))\n",
        "print(confusion_matrix(test_data.label, sig_preds))\n",
        "\n",
        "results_df = pd.DataFrame({\"site\":test_data.source, \"ground_truth\":test_data.label, \"predicted\":sig_preds.flatten()})\n",
        "### Check until here\n",
        "\n",
        "results_df[\"correct\"] = np.where(results_df.ground_truth == results_df.predicted, 1, 0)\n",
        "results_df = results_df.drop(columns=['ground_truth', 'predicted'])\n",
        "\n",
        "site_stats = results_df.groupby([\"site\"]).agg(['count', 'sum'])\n",
        "site_stats.columns = ['_'.join(col) for col in site_stats.columns.values]\n",
        "site_stats.reset_index(inplace=True)\n",
        "site_stats.rename(columns={\"correct_count\":\"support\",\"correct_sum\":\"correct\"}, inplace=True)\n",
        "site_stats[\"accuracy\"] = 100 * site_stats.correct/site_stats.support\n",
        "\n",
        "site_stats_50 = site_stats[site_stats.support > 50]\n",
        "\n",
        "top_10_sites = site_stats_50.sort_values(by=['accuracy'], ascending=False).iloc[:10]\n",
        "\n",
        "for index, row in top_10_sites.iterrows():\n",
        "  d = {\"site\":str(row.site), \"accuracy\":str(row.accuracy), \"support\":str(row.support)}\n",
        "  print(json.dumps(d)+\",\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 38312/38312 [00:23<00:00, 1602.82it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.78      0.74     25859\n",
            "           1       0.41      0.32      0.36     12453\n",
            "\n",
            "    accuracy                           0.63     38312\n",
            "   macro avg       0.56      0.55      0.55     38312\n",
            "weighted avg       0.61      0.63      0.62     38312\n",
            "\n",
            "[[20105  5754]\n",
            " [ 8467  3986]]\n",
            "{\"site\": \"www.yahoo.com\", \"accuracy\": \"77.75061124694376\", \"support\": \"24131\"},\n",
            "{\"site\": \"news.morningstar.com\", \"accuracy\": \"66.87116564417178\", \"support\": \"815\"},\n",
            "{\"site\": \"finance.yahoo.com\", \"accuracy\": \"57.142857142857146\", \"support\": \"63\"},\n",
            "{\"site\": \"www.topstocksforum.com\", \"accuracy\": \"53.44827586206897\", \"support\": \"58\"},\n",
            "{\"site\": \"googlejuices.com\", \"accuracy\": \"52.60663507109005\", \"support\": \"211\"},\n",
            "{\"site\": \"www.financialnewsusa.com\", \"accuracy\": \"51.92307692307692\", \"support\": \"52\"},\n",
            "{\"site\": \"www.investors.com\", \"accuracy\": \"45.12820512820513\", \"support\": \"4290\"},\n",
            "{\"site\": \"investingnews.com\", \"accuracy\": \"32.12996389891697\", \"support\": \"831\"},\n",
            "{\"site\": \"seekingalpha.com\", \"accuracy\": \"30.571227080394923\", \"support\": \"5672\"},\n",
            "{\"site\": \"www.thestreet.com\", \"accuracy\": \"25.18610421836228\", \"support\": \"806\"},\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gL4JJc6G6TEW"
      },
      "source": [
        "from colabcode import ColabCode\n",
        "server = ColabCode(port=10000, code=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-l1qYBq6T3g",
        "outputId": "431dd249-9631-49bd-f5f3-d4fb54926374"
      },
      "source": [
        "server.run_app(app)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Public URL: NgrokTunnel: \"http://565aa61d8efe.ngrok.io\" -> \"http://localhost:10000\"\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:     Started server process [57]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://127.0.0.1:10000 (Press CTRL+C to quit)\n",
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [57]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7Gv7IyT9cPA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}