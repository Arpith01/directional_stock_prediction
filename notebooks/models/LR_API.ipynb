{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LR_API.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arpith01/directional_stock_prediction/blob/master/notebooks/models/LR_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GLjPuAR4fji"
      },
      "source": [
        "!pip install colabcode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcvF3aVK4jg0",
        "outputId": "038e79c8-4073-43cb-bb3f-1968cfbd9aff"
      },
      "source": [
        "!pip install fastapi"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fastapi\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/33/1b643f650688ad368983bbaf3b0658438038ea84d775dd37393d826c3833/fastapi-0.63.0-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 2.7MB/s \n",
            "\u001b[?25hCollecting starlette==0.13.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c5/a4/c9e228d7d47044ce4c83ba002f28ff479e542455f0499198a3f77c94f564/starlette-0.13.6-py3-none-any.whl (59kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 5.4MB/s \n",
            "\u001b[?25hCollecting pydantic<2.0.0,>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/a3/0ffdb6c63f45f10d19b8e8b32670b22ed089cafb29732f6bf8ce518821fb/pydantic-1.8.1-cp37-cp37m-manylinux2014_x86_64.whl (10.1MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1MB 20.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from pydantic<2.0.0,>=1.0.0->fastapi) (3.7.4.3)\n",
            "Installing collected packages: starlette, pydantic, fastapi\n",
            "Successfully installed fastapi-0.63.0 pydantic-1.8.1 starlette-0.13.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RPU3_n24pK3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7bbb57a-4a7c-4891-fc83-c3575f479b3a"
      },
      "source": [
        "import unicodedata\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.downloader.download('vader_lexicon')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import LancasterStemmer,PorterStemmer\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from tqdm import tqdm\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "from nltk.stem import LancasterStemmer,PorterStemmer\n",
        "\n",
        "\n",
        "def nonAsciiChar(words):\n",
        "    words_list=[]\n",
        "    for w in words:\n",
        "        w=re.sub('[^a-zA-Z]+','',re.sub(r'[\\W\\d]','',w.lower()))\n",
        "        format_words=unicodedata.normalize('NFKD', w).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "        words_list.append(format_words)\n",
        "        \n",
        "    return words_list\n",
        "def stemWordsRemoval(words):\n",
        "    stemmer = PorterStemmer()\n",
        "    words_list=[]\n",
        "    for word in words:\n",
        "        word=stemmer.stem(word)\n",
        "        if word not in words_list:\n",
        "            words_list.append(word)\n",
        "    return words_list\n",
        "\n",
        "def stopWordsRemoval(words):\n",
        "    words_list=[]\n",
        "    for w in words:\n",
        "        if w not in stopwords:\n",
        "            words_list.append(w)\n",
        "    return words_list\n",
        "\n",
        "def removeLinks(words):\n",
        "    words_list=[]\n",
        "    for w in words:\n",
        "        if not re.match('[www]',w):\n",
        "            words_list.append(w)\n",
        "    return words_list\n",
        "\n",
        "def spaceRemoval(words):\n",
        "    words_list=[]\n",
        "    for w in words:\n",
        "        if w!='':\n",
        "            words_list.append(w)\n",
        "    return words_list\n",
        "\n",
        "\n",
        "def dataExtraction(words):\n",
        "    words=nonAsciiChar(words)\n",
        "    words=spaceRemoval(words)\n",
        "    words=stopWordsRemoval(words)\n",
        "    words=stemWordsRemoval(words)\n",
        "    words=removeLinks(words)\n",
        "    return words"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opwDmjVQ4yZc"
      },
      "source": [
        "def get_sentences(news_article):\n",
        "    paragraphs = nltk.sent_tokenize(news_article.lower())\n",
        "    sentences=[]\n",
        "    is_amazon = []\n",
        "    amzn_sentences = []\n",
        "    apple_sentences = []\n",
        "    key_words=['amazon','apple','aapl','amzn']\n",
        "    amazon_keywords = ['amazon', 'amzn']\n",
        "    apple_keywords = ['apple', 'aapl']\n",
        "\n",
        "    for para in paragraphs:\n",
        "      sentences_list = para.split(\"\\n\")\n",
        "      for sentence in sentences_list:\n",
        "        if any(key in sentence for key in amazon_keywords):\n",
        "          sentences.append(sentence)\n",
        "          is_amazon.append(1)\n",
        "        if any(key in sentence for key in apple_keywords):\n",
        "          sentences.append(sentence)\n",
        "          is_amazon.append(0)\n",
        "          # if any(key in sentence for key in key_words):\n",
        "          #   sentences.append(sentence)\n",
        "\n",
        "\n",
        "    if len(sentences) == 0:\n",
        "      return None\n",
        "    \n",
        "    sentences_df = pd.DataFrame({'text':sentences, 'is_amazon':is_amazon})\n",
        "\n",
        "    sentences_df=sentences_df[sentences_df['text'].str.match('^[A-Z a-z 0-9]+')]\n",
        "\n",
        "    sentences_df=sentences_df.drop_duplicates(keep=False).reset_index(drop=True)\n",
        "\n",
        "    sentences_df['words'] = sentences_df.text.apply(word_tokenize)\n",
        "    sentences_df['words'] = sentences_df.words.apply(dataExtraction)\n",
        "    sentences_df['text'] = sentences_df.words.apply(lambda words: \" \".join(words))\n",
        "    sentences_df = sentences_df.drop(columns=\"words\")\n",
        "\n",
        "    return sentences_df"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfTZUVKe47HO"
      },
      "source": [
        "from pydantic import BaseModel\n",
        "from fastapi import FastAPI"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27ozejKW495A"
      },
      "source": [
        "from fastapi.middleware.cors import CORSMiddleware"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7r_rD8t5Aav"
      },
      "source": [
        "class Sentence(BaseModel):\n",
        "  news_article:str"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7UY81Zp5CY6"
      },
      "source": [
        "class Prediction(BaseModel):\n",
        "  prediction_apple:str\n",
        "  confidence_apple:str\n",
        "  prediction_amazon:str\n",
        "  confidence_amazon:str"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiVwdgpb5EG5"
      },
      "source": [
        "app = FastAPI()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pf5379tT5HgQ"
      },
      "source": [
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWKMuHGf825b",
        "outputId": "5884fb81-c59b-4388-97e2-fce4139875df"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZp5X5HoMZD6"
      },
      "source": [
        "##Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVJ0s3R780iE",
        "outputId": "a7b41ec0-40d6-4a03-c65b-bc10a610c208"
      },
      "source": [
        "import pickle\n",
        "path = \"/content/drive/MyDrive/new_data_ARS/log_reg.sav\"\n",
        "infile = open(path,'rb')\n",
        "SVC_Linear = pickle.load(infile)\n",
        "infile.close()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.23.2 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
            "  UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inCiVPcV5RKi",
        "outputId": "28233527-e69c-4d3b-88ee-d9b9096e384b"
      },
      "source": [
        "saved_model_path = SVC_Linear\n",
        "saved_model_path"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=0, solver='liblinear', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blxNHPhBJZC3"
      },
      "source": [
        "path = \"/content/drive/MyDrive/new_data_ARS/vectorizer\"\n",
        "infile = open(path,'rb')\n",
        "vectorizer_dict = pickle.load(infile)\n",
        "infile.close()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhqy-CpOKjtX"
      },
      "source": [
        "vectorizer = vectorizer_dict['vectorizer']"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3rf5W7gKuUq",
        "outputId": "1ca71456-9183-4e98-ab02-61676db145a8"
      },
      "source": [
        "vectorizer"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
              "                lowercase=True, max_df=1.0, max_features=4000, min_df=10,\n",
              "                ngram_range=(2, 2), preprocessor=None, stop_words=None,\n",
              "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                tokenizer=None, vocabulary=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4OnyV2KCEdT"
      },
      "source": [
        "def get_features(X_test):\n",
        "  cls = SentimentIntensityAnalyzer()\n",
        "  Positive_X_test_text = []\n",
        "  Negative_X_test_text = []\n",
        "\n",
        "  for i in tqdm(X_test):\n",
        "      n1= cls.polarity_scores(i)[\"neg\"]\n",
        "      n2= cls.polarity_scores(i)[\"pos\"]\n",
        "      Positive_X_test_text.append(n1)\n",
        "      Negative_X_test_text.append(n2)\n",
        "  \n",
        "  # vectorizer = CountVectorizer(ngram_range = (2,2),max_features = 4000, min_df = 10)\n",
        "  \n",
        "  X_test_2gram_features = vectorizer.transform(X_test)\n",
        "  Sentiment_X_test_text = np.column_stack((Positive_X_test_text, Negative_X_test_text))\n",
        "\n",
        "  X_test_2gram_features_array = X_test_2gram_features.toarray()\n",
        "  Xtest = np.concatenate((Sentiment_X_test_text,X_test_2gram_features_array), axis = 1)\n",
        "  return Xtest"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaaLKe-55Jb9"
      },
      "source": [
        "@app.on_event(\"startup\")\n",
        "def load_model_from_file():\n",
        "  global inference_model\n",
        "  inference_model = saved_model_path\n",
        "\n",
        "\n",
        "@app.get('/')\n",
        "def index():\n",
        "  return{'message':'Post an article to this API to get inference'}\n",
        "\n",
        "@app.post('/predict')\n",
        "def classify_sentence(data:Sentence):\n",
        "  received_data = data.dict()\n",
        "  news_article = received_data['news_article']\n",
        "  sentences = get_sentences(news_article)\n",
        "  prediction_class_amzn = \"-1\"\n",
        "  prediction_class_apple = \"-1\"\n",
        "  confidence_amzn = '0'\n",
        "  confidence_apple = '0'\n",
        "\n",
        "  if sentences is None:\n",
        "    p = Prediction(prediction_apple=prediction_class_apple, confidence_apple=confidence_apple, prediction_amazon = prediction_class_amzn, confidence_amazon=confidence_amzn)\n",
        "    return p\n",
        "\n",
        "\n",
        "  amazon_sentences = sentences[sentences['is_amazon'] == 1].text\n",
        "  apple_sentences = sentences[sentences['is_amazon'] == 0].text\n",
        "\n",
        "  # posterior_prob = tf.sigmoid(inference_model.predict(sentences))\n",
        "\n",
        "  if(len(amazon_sentences) != 0):\n",
        "    amazon_sentences_test = get_features(amazon_sentences)\n",
        "    posterior_prob_amzn = inference_model.predict(amazon_sentences_test)\n",
        "    prediction_class_amzn = np.where(posterior_prob_amzn>0.5, 1, 0)\n",
        "    bin_count_amzn = np.bincount(prediction_class_amzn.flatten())\n",
        "\n",
        "    prediction_class_amzn = np.bincount(prediction_class_amzn.flatten()).argmax()\n",
        "\n",
        "    confidence_amzn = bin_count_amzn[prediction_class_amzn]/np.sum(bin_count_amzn)\n",
        "    confidence_amzn = str(confidence_amzn)\n",
        "    prediction_class_amzn = str(prediction_class_amzn)\n",
        "\n",
        "\n",
        "  if(len(apple_sentences) != 0):\n",
        "    apple_sentences_test = get_features(apple_sentences)\n",
        "    posterior_prob_apple = inference_model.predict(apple_sentences_test)\n",
        "    prediction_class_apple = np.where(posterior_prob_apple>0.5, 1, 0)\n",
        "    bin_count_apple = np.bincount(prediction_class_apple.flatten())\n",
        "\n",
        "    prediction_class_apple = np.bincount(prediction_class_apple.flatten()).argmax()\n",
        "\n",
        "    confidence_apple = bin_count_apple[prediction_class_apple]/np.sum(bin_count_apple)\n",
        "    prediction_class_apple = str(prediction_class_apple)\n",
        "    confidence_apple = str(confidence_apple)\n",
        "\n",
        "  # prediction_classes = np.where(posterior_prob>0.5, 1, 0)\n",
        "\n",
        "  # print(prediction_classes)\n",
        "  # prediction_class = np.bincount(prediction_classes.flatten()).argmax()\n",
        "  # p = Prediction(prediction=str(prediction_class))\n",
        "  p = Prediction(prediction_apple=prediction_class_apple, confidence_apple=confidence_apple, prediction_amazon = prediction_class_amzn, confidence_amazon=confidence_amzn)\n",
        "\n",
        "  return p"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gL4JJc6G6TEW"
      },
      "source": [
        "from colabcode import ColabCode\n",
        "server = ColabCode(port=10000, code=False)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-l1qYBq6T3g",
        "outputId": "04de7847-de96-4726-df02-2b65747e703e"
      },
      "source": [
        "server.run_app(app)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:     Started server process [58]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://127.0.0.1:10000 (Press CTRL+C to quit)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Public URL: NgrokTunnel: \"http://d26ce0477fb3.ngrok.io\" -> \"http://localhost:10000\"\n",
            "INFO:     72.223.1.23:0 - \"GET / HTTP/1.1\" 200 OK\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 385.01it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:     72.223.1.23:0 - \"POST /predict HTTP/1.1\" 200 OK\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 1/1 [00:00<00:00, 216.19it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:     72.223.1.23:0 - \"POST /predict HTTP/1.1\" 200 OK\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 1/1 [00:00<00:00, 606.20it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 563.60it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:     72.223.1.23:0 - \"POST /predict HTTP/1.1\" 200 OK\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 1/1 [00:00<00:00, 584.25it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 746.32it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:     72.223.1.23:0 - \"POST /predict HTTP/1.1\" 200 OK\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 11/11 [00:00<00:00, 2227.24it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:     72.223.1.23:0 - \"POST /predict HTTP/1.1\" 200 OK\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 1/1 [00:00<00:00, 273.98it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:     72.223.1.23:0 - \"POST /predict HTTP/1.1\" 200 OK\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 1/1 [00:00<00:00, 250.80it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:     72.223.1.23:0 - \"POST /predict HTTP/1.1\" 200 OK\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 1/1 [00:00<00:00, 92.36it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:     72.223.1.23:0 - \"POST /predict HTTP/1.1\" 200 OK\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 1/1 [00:00<00:00, 609.02it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:     72.223.1.23:0 - \"POST /predict HTTP/1.1\" 200 OK\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7Gv7IyT9cPA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}